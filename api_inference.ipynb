{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q ollama\n",
    "!pip install -r requirements.txt\n",
    "!pip install -q tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install glair sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "# Set environment variables for Git credentials\n",
    "username = getpass.getpass(\"Input Your GitHub Username: \")\n",
    "token = getpass.getpass(\"Input Your GitHub Personal Access Token: \")\n",
    "\n",
    "os.environ['GIT_USERNAME'] = username\n",
    "os.environ['GIT_TOKEN'] = token\n",
    "\n",
    "# Clone using the credential helper\n",
    "!git config --global credential.helper store\n",
    "!echo \"https://$GIT_USERNAME:$GIT_TOKEN@github.com\" > ~/.git-credentials\n",
    "!git clone https://github.com/GDP-ADMIN/gen-ai-internal.git\n",
    "!cd gen-ai-internal/libs/gllm-retrieval && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_sdk_library():\n",
    "    token = getpass.getpass(\"Input Your Personal Access Token: \")\n",
    "\n",
    "    cmd = f'pip install \"gllm-retrieval @ git+https://{token}@github.com/GDP-ADMIN/gen-ai-internal.git@f/gllm-retriever-text-to-sql#subdirectory=libs/gllm-retrieval\"'\n",
    "\n",
    "    with subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) as process:\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            sys.stdout.write(stderr)\n",
    "            raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
    "        else:\n",
    "            sys.stdout.write(stdout)\n",
    "\n",
    "install_sdk_library()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model for text to sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_SPREADSHEET_ID: str = \"1dDMqrol_DrEMjvLy88IRu2WdHN7T5BU0LrD8ORLuNPI\" # put your spreadsheet id here\n",
    "GOOGLE_SPREADSHEET_URL: str = f\"https://docs.google.com/spreadsheets/d/{GOOGLE_SPREADSHEET_ID}/edit?usp=sharing\" # put your spreadsheet link here\n",
    "DATA_TEST_SHEET_NAME: str = \"catapa_test_core_employee\"\n",
    "\n",
    "GOOGLE_SHEETS_CLIENT_EMAIL: str = os.getenv('GOOGLE_SHEETS_CLIENT_EMAIL')\n",
    "GOOGLE_SHEETS_PRIVATE_KEY: str = os.getenv('GOOGLE_SHEETS_PRIVATE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Auth\n",
    "# Google Authentication\n",
    "from modules.google_sheets_writer import GoogleUtil\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "PRIVATE_KEY = GOOGLE_SHEETS_PRIVATE_KEY\n",
    "google: GoogleUtil = GoogleUtil(PRIVATE_KEY, GOOGLE_SHEETS_CLIENT_EMAIL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Category</th>\n",
       "      <th>Expected SQL Query</th>\n",
       "      <th>Expected Query Result</th>\n",
       "      <th>Expected SQL Query (base from catapa)</th>\n",
       "      <th>Expected Query Result (base from catapa)</th>\n",
       "      <th>Catapa Prompts</th>\n",
       "      <th>Database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bagaimana perbandingan jumlah karyawan berdasa...</td>\n",
       "      <td>medium</td>\n",
       "      <td>SELECT organizations.name AS \"organization_nam...</td>\n",
       "      <td>[{'organization_name': 'Information Technology...</td>\n",
       "      <td>SELECT\\n        organizations.name AS organiza...</td>\n",
       "      <td>[{'organization_name': 'ICT', 'total_employees...</td>\n",
       "      <td>System: **System Instruction**:\\n - You are a ...</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bagaimana data termination berdasarkan nama ja...</td>\n",
       "      <td>medium</td>\n",
       "      <td>SELECT job_titles.name AS \"job_title_name\", CO...</td>\n",
       "      <td>[{'job_title_name': 'Information Technology', ...</td>\n",
       "      <td>SELECT\\n        job_titles.name AS job_title_n...</td>\n",
       "      <td>[{'job_title': 'HRD - Finance', 'termination_c...</td>\n",
       "      <td>System: **System Instruction**:\\n - You are a ...</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No                                             Prompt Category  \\\n",
       "0  1  Bagaimana perbandingan jumlah karyawan berdasa...   medium   \n",
       "1  2  Bagaimana data termination berdasarkan nama ja...   medium   \n",
       "\n",
       "                                  Expected SQL Query  \\\n",
       "0  SELECT organizations.name AS \"organization_nam...   \n",
       "1  SELECT job_titles.name AS \"job_title_name\", CO...   \n",
       "\n",
       "                               Expected Query Result  \\\n",
       "0  [{'organization_name': 'Information Technology...   \n",
       "1  [{'job_title_name': 'Information Technology', ...   \n",
       "\n",
       "               Expected SQL Query (base from catapa)  \\\n",
       "0  SELECT\\n        organizations.name AS organiza...   \n",
       "1  SELECT\\n        job_titles.name AS job_title_n...   \n",
       "\n",
       "            Expected Query Result (base from catapa)  \\\n",
       "0  [{'organization_name': 'ICT', 'total_employees...   \n",
       "1  [{'job_title': 'HRD - Finance', 'termination_c...   \n",
       "\n",
       "                                      Catapa Prompts Database  \n",
       "0  System: **System Instruction**:\\n - You are a ...     core  \n",
       "1  System: **System Instruction**:\\n - You are a ...     core  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load Data Test\n",
    "rows: List[list] = google.retrieve_worksheet(GOOGLE_SPREADSHEET_ID, DATA_TEST_SHEET_NAME)\n",
    "df_data_test: pd.DataFrame = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "display(df_data_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MariaDB Server version 5.5.5-10.5.28-MariaDB-ubu2004\n",
      "Connected to MariaDB Server version 5.5.5-10.5.28-MariaDB-ubu2004\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "\n",
    "def connect_to_mariadb(database_name: str) -> Optional[mysql.connector.connection.MySQLConnection]:\n",
    "    \"\"\"\n",
    "    Connect to the MariaDB database.\n",
    "\n",
    "    Returns:\n",
    "        Optional[mysql.connector.connection.MySQLConnection]: A connection object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=\"localhost\",\n",
    "            port=33062,\n",
    "            user=\"app_user_demo\",\n",
    "            password=\"StrongPassw0rd!\",\n",
    "            database=database_name\n",
    "        )\n",
    "\n",
    "        if connection.is_connected():\n",
    "            db_info = connection.get_server_info()\n",
    "            print(f\"Connected to MariaDB Server version {db_info}\")\n",
    "            return connection\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to MariaDB: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_query(connection: mysql.connector.connection.MySQLConnection, query: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Execute a query on the MariaDB database.\n",
    "\n",
    "    Args:\n",
    "        connection: The database connection object.\n",
    "        query: The SQL query to execute.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries containing the query results.\n",
    "    \"\"\"\n",
    "    cursor = connection.cursor(dictionary=True)\n",
    "    cursor.execute(query)\n",
    "    result = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    return result\n",
    "\n",
    "time_management_db = connect_to_mariadb(\"ru4f_time_management\")\n",
    "core_employee_db = connect_to_mariadb(\"ru4f_core_employee\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using API endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET OPEN_AI/DEEPSEEK CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gllm_inference.lm_invoker.openai_compatible_lm_invoker import OpenAICompatibleLMInvoker\n",
    "import os\n",
    "\n",
    "OPEN_AI_KEY = os.getenv(\"OPEN_AI_KEY\")\n",
    "OPENAI_ENDPOINT = os.getenv(\"OPENAI_ENDPOINT\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\")\n",
    "\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "DEEPSEEK_ENDPOINT = os.getenv(\"DEEPSEEK_ENDPOINT\")\n",
    "DEEPSEEK_MODEL = os.getenv(\"DEEPSEEK_MODEL\")\n",
    "\n",
    "\n",
    "lm_invoker = OpenAICompatibleLMInvoker(\n",
    "    base_url=OPENAI_ENDPOINT,\n",
    "    model_name=OPENAI_MODEL,\n",
    "    api_key=OPEN_AI_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"**System Instruction**:\n",
    "- You are a SQL generator expert.\n",
    "- Your role is to create SQL queries based on provided database schema, table relationships, master data, the current date, and a user's instruction.\n",
    "- If the query uses any table listed in `Data Trustee Enabled Tables`, you must modify the query to include data trustee compliance requirements. Otherwise, you must not add any JOIN or filter related to data trustee compliance.\n",
    "\n",
    "**Database Schema**:\n",
    "- The schema for the database is as follows:\n",
    "  {schema}\n",
    "\n",
    "**Table Relationships**:\n",
    "- The relationships between these tables are described below:\n",
    "  {relations}\n",
    "\n",
    "**Master Data**:\n",
    "- Master data in the database are listed here:\n",
    "  {master_data}\n",
    "\n",
    "**Data Trustee Enabled Tables**:\n",
    "- List of tables requiring a data trustee and specific columns for JOIN operations. Entries may indicate a prerequisite join with another table before joining to the `employment_statuses` table, denoted by `table_name.column_name`. A join must first occur with `table_name` then followed by a join to `employment_statuses` using `column_name` from `table_name`:\n",
    "  {data_trustee_tables}\n",
    "\n",
    "**Anonymized Entities**:\n",
    "- List of anonymized entities along with their descriptions that may be used in the `WHERE` clause:\n",
    "  {anonymized_entities_description}\n",
    "\n",
    "**Query Examples**:\n",
    "- Here are the query examples for some cases:\n",
    "\n",
    "> Turnover Rate:\n",
    "Calculate the turnover rate for each year over the last 10 years. When querying turnover rate, do not filter the approval status in the WHERE clause.\n",
    "query: WITH year_series AS (\n",
    "SELECT YEAR(DATE_SUB(CURRENT_DATE, INTERVAL n YEAR)) AS year\n",
    "FROM (\n",
    "    SELECT 0 AS n UNION ALL SELECT 1 UNION ALL SELECT 2 UNION ALL SELECT 3 UNION ALL SELECT 4\n",
    "    UNION ALL SELECT 5 UNION ALL SELECT 6 UNION ALL SELECT 7 UNION ALL SELECT 8 UNION ALL SELECT 9\n",
    ") numbers\n",
    ")\n",
    "SELECT\n",
    "ys.year AS `Year`,\n",
    "SUM(\n",
    "CASE\n",
    "    WHEN te.effective_date BETWEEN STR_TO_DATE(CONCAT(ys.year, '-01-01'), '%Y-%m-%d')\n",
    "        AND STR_TO_DATE(CONCAT(ys.year, '-12-31'), '%Y-%m-%d')\n",
    "        AND te.approval_status = 'APPROVED'\n",
    "    THEN 1\n",
    "    ELSE 0\n",
    "END\n",
    ") / SUM(\n",
    "CASE\n",
    "    WHEN employees.join_date <= STR_TO_DATE(CONCAT(ys.year, '-01-01'), '%Y-%m-%d')\n",
    "        AND (\n",
    "            te.id IS NULL\n",
    "            OR te.effective_date >= STR_TO_DATE(CONCAT(ys.year, '-01-01'), '%Y-%m-%d')\n",
    "            OR te.approval_status != 'APPROVED'\n",
    "        )\n",
    "    THEN 1\n",
    "    ELSE 0\n",
    "END\n",
    ") * 100 AS `Turnover Rate`\n",
    "FROM\n",
    "employees\n",
    "JOIN employment_statuses ON employees.id = employment_statuses.employee_id\n",
    "CROSS JOIN\n",
    "year_series ys\n",
    "LEFT JOIN\n",
    "termination_entries te ON employees.id = te.employee_id\n",
    "WHERE employment_statuses.organization_id IN ('[ORGANIZATION_IDS]')\n",
    "AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]')\n",
    "AND employment_statuses.location_id IN ('[LOCATION_IDS]')\n",
    "GROUP BY\n",
    "ys.year;\n",
    "\n",
    "Retrieve the turnover rate for the current year by default if the user does not specify a turnover rate period.\n",
    "query: SELECT\n",
    "    YEAR(CURRENT_DATE) AS `Year`,\n",
    "    (SUM(\n",
    "        CASE\n",
    "            WHEN te.effective_date BETWEEN STR_TO_DATE(CONCAT(YEAR(CURRENT_DATE), '-01-01'), '%Y-%m-%d')\n",
    "                AND STR_TO_DATE(CONCAT(YEAR(CURRENT_DATE), '-12-31'), '%Y-%m-%d')\n",
    "                AND te.approval_status = 'APPROVED'\n",
    "        THEN 1\n",
    "        ELSE 0\n",
    "    END\n",
    "    ) / SUM(\n",
    "        CASE\n",
    "            WHEN employees.join_date <= STR_TO_DATE(CONCAT(YEAR(CURRENT_DATE), '-01-01'), '%Y-%m-%d')\n",
    "                AND (\n",
    "                    te.id IS NULL\n",
    "                    OR te.effective_date >= STR_TO_DATE(CONCAT(YEAR(CURRENT_DATE), '-01-01'), '%Y-%m-%d')\n",
    "                    OR te.approval_status != 'APPROVED'\n",
    "                )\n",
    "        THEN 1\n",
    "        ELSE 0\n",
    "    END\n",
    "    )) * 100 AS `Turnover Rate`\n",
    "FROM\n",
    "    employees\n",
    "JOIN employment_statuses ON employees.id = employment_statuses.employee_id\n",
    "LEFT JOIN\n",
    "    termination_entries te ON employees.id = te.employee_id\n",
    "WHERE employment_statuses.organization_id IN ('[ORGANIZATION_IDS]')\n",
    "AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]')\n",
    "AND employment_statuses.location_id IN ('[LOCATION_IDS]');\n",
    "\n",
    "> Quarterly new employee count:\n",
    "Calculate the number of employees hired each quarter.\n",
    "query: SELECT\n",
    "  YEAR(employees.join_date) AS `Year`,\n",
    "  QUARTER(employees.join_date) AS `Quarter`,\n",
    "  COUNT(*) AS `New Employee Count`\n",
    "FROM\n",
    "  employees\n",
    "JOIN employment_statuses ON employees.id = employment_statuses.employee_id\n",
    "WHERE\n",
    "  employees.active = TRUE\n",
    "  AND employment_statuses.organization_id IN ('[ORGANIZATION_IDS]')\n",
    "  AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]')\n",
    "  AND employment_statuses.location_id IN ('[LOCATION_IDS]')\n",
    "GROUP BY\n",
    "  YEAR(employees.join_date),\n",
    "  QUARTER(employees.join_date)\n",
    "ORDER BY\n",
    "  `Year`,\n",
    "  `Quarter`;\n",
    "\n",
    "> Employee Managerial Status:\n",
    "Display all employees' names along with their managerial status (Manager/Non-Manager).\n",
    "query: SELECT\n",
    "  employees.name AS 'Employee Name',\n",
    "  CASE WHEN employees.id IN (SELECT manager_id FROM employees) THEN 'Manager' ELSE 'Non-Manager' END AS 'Managerial Status'\n",
    "FROM\n",
    "  employees\n",
    "JOIN employment_statuses ON employees.id = employment_statuses.employee_id\n",
    "WHERE\n",
    "  employees.active = TRUE\n",
    "  AND employment_statuses.organization_id IN ('[ORGANIZATION_IDS]')\n",
    "  AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]')\n",
    "  AND employment_statuses.location_id IN ('[LOCATION_IDS]');\n",
    "\n",
    "> Attendance After Holiday:\n",
    "Show the attendance status of all employees the day after a holiday.\n",
    "query: SELECT\n",
    "  employees.name,\n",
    "  attendance_statuses.name,\n",
    "  attendances.date\n",
    "FROM\n",
    "  attendances\n",
    "  JOIN employees ON attendances.employee_id = employees.id\n",
    "  JOIN attendance_statuses ON attendances.attendance_status_in_id = attendance_statuses.id\n",
    "  JOIN employment_statuses ON employees.id = employment_statuses.employee_id\n",
    "WHERE\n",
    "  attendances.date IN (\n",
    "    SELECT\n",
    "      date + INTERVAL 1 DAY\n",
    "    FROM\n",
    "      employee_roster_view\n",
    "    WHERE\n",
    "      shift_id IS NULL\n",
    "  )\n",
    "AND employees.active = TRUE\n",
    "AND employment_statuses.organization_id IN ('[ORGANIZATION_IDS]')\n",
    "AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]')\n",
    "AND employment_statuses.location_id IN ('[LOCATION_IDS]');\n",
    "\n",
    "> Job Titles List:\n",
    "List all job titles.\n",
    "query: SELECT\n",
    "  job_titles.name AS `Job Title`\n",
    "FROM\n",
    "  job_titles;\n",
    "\n",
    "Explanation:\n",
    "- In this case, the `job_titles` table is not listed in the `Data Trustee Enabled Tables`. Therefore, no JOIN or filter related to data trustee compliance is added, and the query remains simple.\n",
    "\n",
    "> Example of Query Requiring Prior Join:\n",
    "Retrieve all loan installments.\n",
    "Explanation: The `loan_installments` table requires a prior join with the `loan_entries` table before joining with the `employment_statuses` table for data trustee compliance.\n",
    "query: SELECT * FROM loan_installments\n",
    " JOIN loan_entries ON loan_installments.loan_entry_id = loan_entries.id\n",
    " JOIN employment_statuses ON loan_entries.employee_id = employment_statuses.employee_id\n",
    "WHERE employment_statuses.organization_id IN ('[ORGANIZATION_IDS]')\n",
    "AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]')\n",
    "AND employment_statuses.location_id IN ('[LOCATION_IDS]');\n",
    "\n",
    "**Current Date**:\n",
    "- The current date is provided for context:\n",
    "  {current_date}\n",
    "\n",
    "**Definitions**:\n",
    "- Quarter: A quarter is a three-month period on a financial calendar.\n",
    "  First quarter: January 1 - March 31\n",
    "  Second quarter: April 1 - June 30\n",
    "  Third quarter: July 1 - September 30\n",
    "  Fourth quarter: October 1 - December 31\n",
    "- Semester: A semester is a six-month period on a financial calendar.\n",
    "  First semester: January 1 - June 30\n",
    "  Second semester: July 1 - December 31\n",
    "\n",
    "**User's Instruction**:\n",
    "- Below is the user's instruction:\n",
    "  {user_instruction}\n",
    "\n",
    "**Task**:\n",
    "1. **Generate SQL Query**:\n",
    "   - Generate an SQL query that aggregates data relevant to the user's instruction.\n",
    "\n",
    "2. **Check for Data Trustee Requirements**:\n",
    "   - If the query uses any table listed in `Data Trustee Enabled Tables`, modify the query to:\n",
    "     - Add a JOIN operation with the `employment_statuses` table using the `employee_id` column.\n",
    "     - Add filters for `employment_statuses.organization_id IN ('[ORGANIZATION_IDS]'), employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]'), employment_statuses.location_id IN ('[LOCATION_IDS]')`.\n",
    "     - Ensure prerequisite JOINs are added before joining with the `employment_statuses` table as per the data trustee table list.\n",
    "   - If the query does not use any table in the `Data Trustee Enabled Tables`, do not add the data trustee modifications.\n",
    "\n",
    "**Specific SQL Requirements**:\n",
    "- The SQL query should:\n",
    "   - Be executable directly in MariaDB 10.4.\n",
    "   - Do NOT `SELECT` any identifiers like `id` or `employee_id`, etc., except for aggregate functions like `MAX`, `SUM`, `AVG`, etc.\n",
    "   - Always show name instead of id, for example use religion.name instead of religion.id. If the name is not directly available, join to the respective table to get the name.\n",
    "   - Always prefix column names with the table name to avoid ambiguity and ensure clarity.\n",
    "   - Use aliases in the `SELECT`, clause using snake_case.\n",
    "   - Do NOT use aliases in `FROM` clause. e.g `FROM employees AS e` is not allowed.\n",
    "   - Correctly handle aggregate functions like `MAX`, `SUM`, `AVG`, etc., within the `HAVING` or `SELECT` clauses, not in the `WHERE` clause.\n",
    "   - Ensure that JOIN conditions match the correct foreign keys and are structured properly.\n",
    "   - Always wrap date string in `STR_TO_DATE()` function. Example: STR_TO_DATE('2020-01-01', '%Y-%m-%d').\n",
    "   - When comparing dates or using date ranges, ensure the comparator is in date type by using `CAST()`. Example:\n",
    "     ```sql\n",
    "     WHERE attendance.date BETWEEN CAST(DATE_FORMAT(CURRENT_DATE, '%Y-01-01') AS DATE)\n",
    "       AND CAST(DATE_FORMAT(CURRENT_DATE, '%Y-12-31') AS DATE)\n",
    "     ```\n",
    "   - When trying to find the last day of some period, avoid using `LAST_DAY()` function.\n",
    "   - Include conditional logic based on table data:\n",
    "     - If including data from \"employees\" but not \"termination_entries\", add:\n",
    "       `\"WHERE employees.active = TRUE\"`\n",
    "     - If including data from \"termination_entries\", add:\n",
    "       `\"WHERE termination_entries.approval_status = 'APPROVED'\"`\n",
    "     - Avoid combining both `WHERE` statements in the same query.\n",
    "  - Ensure `WHERE` clauses are placed before the `GROUP BY` clause to filter rows before grouping.\n",
    "  - Use `HAVING` clauses after `GROUP BY` to apply conditions on aggregated data such as sums, counts, or max values.\n",
    "  - Do NOT use aliases in `HAVING` clauses.\n",
    "  - Do not `CAST` the column `custom_data.value`.\n",
    "  - When performing division, ensure that the denominator is not zero to avoid division by zero.\n",
    "  - Rename the column names based on the user's instructions, opting for a human-readable format instead of snake case. Make sure the column name and the user question are in the same language.\n",
    "  - Ensure you reference the correct column name using backticks (`).\n",
    "  - When filtering with anonymized values (marked with `<...>`):\n",
    "    - Avoid compare against the `id` column or `id` foreign key column (e.g., `location_id`).\n",
    "    - Anonymized values follow this pattern: `<ENTITY_NAME_NUMBER>` (e.g., `<LOCATION_1>`)\n",
    "    - Examples:\n",
    "        - Correct: `WHERE locations.name = '<LOCATION_1>'`\n",
    "        - Incorrect: `WHERE locations.id = '<LOCATION_1>'` or `WHERE location_id = '<LOCATION_1>'`\n",
    "    - This applies to all anonymized entities listed in `Anonymized Entities` section\n",
    "\n",
    "**SQL Query**:\n",
    "- Your generated and modified SQL query should be placed here.\n",
    "- Do NOT add anything else besides the SQL query.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction sql query using api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from gllm_inference.prompt_builder import OpenAIPromptBuilder\n",
    "from gllm_inference.request_processor import LMRequestProcessor\n",
    "from gllm_retrieval.query_transformer.text_to_sql_query_transformer import OneToOneQueryTransformer\n",
    "from gllm_retrieval.utils.sql_utils import format_sql_query\n",
    "\n",
    "from modules.database_info.schema import employee_schema, time_management_schema\n",
    "from modules.database_info.master_data import employee_master_data, time_management_master_data\n",
    "from modules.database_info.relation import employee_relations, time_management_relations\n",
    "from modules.database_info.trustee_tables import data_trustee_employee, data_trustee_time_management\n",
    "from modules.database_info.anonymize_entities import anonymized_entities_description\n",
    "\n",
    "async def process_sql_queries(\n",
    "    df_data_test: pd.DataFrame,\n",
    "    inference_result_dir: str,\n",
    "    model_name: str,\n",
    "    system_prompt: str,\n",
    "    lm_invoker: Any,\n",
    "    context_data: Dict[str, str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process SQL queries for all test data rows.\n",
    "\n",
    "    Args:\n",
    "        df_data_test: DataFrame containing test data\n",
    "        inference_result_dir: Directory to store results\n",
    "        model_name: Name of the model used for inference\n",
    "        system_prompt: System prompt template\n",
    "        lm_invoker: LM invoker instance\n",
    "        context_data: Dictionary with context data for the prompt\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with query results\n",
    "    \"\"\"\n",
    "    # Ensure result directory exists\n",
    "    os.makedirs(inference_result_dir, exist_ok=True)\n",
    "\n",
    "    # Setup result path and load existing results if available\n",
    "    sql_generator_result_path = os.path.join(inference_result_dir, f\"{model_name}-using-catapa-prompt.csv\")\n",
    "\n",
    "    if os.path.exists(sql_generator_result_path):\n",
    "        df_query_result = pd.read_csv(sql_generator_result_path)\n",
    "        processed_nos = set(df_query_result['No'].astype(int).tolist())\n",
    "    else:\n",
    "        df_query_result = pd.DataFrame(\n",
    "            columns=[\n",
    "                'No',\n",
    "                'Prompt',\n",
    "                'Generated SQL Query',\n",
    "                'Expected SQL Query',\n",
    "                'Expected Query Result',\n",
    "                'Time Taken'\n",
    "            ]\n",
    "        )\n",
    "        processed_nos = set()\n",
    "\n",
    "    # Setup the query transformer\n",
    "    prompt_builder = OpenAIPromptBuilder(system_prompt)\n",
    "    lm_request_processor = LMRequestProcessor(prompt_builder, lm_invoker)\n",
    "    text_to_sql = OneToOneQueryTransformer(lm_request_processor)\n",
    "\n",
    "    # Process each row\n",
    "    new_results = []\n",
    "    with tqdm(total=len(df_data_test), desc=\"Generating SQL queries\") as pbar:\n",
    "        for _, df_row in df_data_test.iterrows():\n",
    "            no = int(df_row['No'])\n",
    "\n",
    "            # Skip already processed items\n",
    "            if no in processed_nos:\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Status\": f\"Skipped {no}\"})\n",
    "                continue\n",
    "\n",
    "            user_instruction = df_row['Prompt']\n",
    "\n",
    "            try:\n",
    "                # Measure time and generate SQL\n",
    "                start_time = time()\n",
    "                queries = await text_to_sql.transform({\n",
    "                    **context_data,\n",
    "                    \"user_instruction\": user_instruction\n",
    "                })\n",
    "                time_taken = time() - start_time\n",
    "\n",
    "                sql_query = format_sql_query(queries[0])\n",
    "\n",
    "                # Create result row\n",
    "                result_row = {\n",
    "                    'No': no,\n",
    "                    'Prompt': df_row['Prompt'],\n",
    "                    'Generated SQL Query': sql_query,\n",
    "                    'Expected SQL Query': df_row['Expected SQL Query'],\n",
    "                    'Expected Query Result': df_row['Expected Query Result'],\n",
    "                    'Time Taken': time_taken\n",
    "                }\n",
    "\n",
    "                new_results.append(result_row)\n",
    "\n",
    "                # Update progress\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Status\": f\"Processed {no}\", \"Time\": f\"{time_taken:.2f}s\"})\n",
    "\n",
    "                # Save every 5 records to prevent data loss\n",
    "                if len(new_results) % 5 == 0:\n",
    "                    temp_df = pd.DataFrame(new_results)\n",
    "                    df_query_result = pd.concat([df_query_result, temp_df], ignore_index=True)\n",
    "                    df_query_result.to_csv(sql_generator_result_path, index=False)\n",
    "\n",
    "            except Exception as e:\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix({\"Status\": f\"Error on {no}\", \"Error\": str(e)[:20]})\n",
    "                print(f\"Error processing query {no}: {str(e)}\")\n",
    "\n",
    "    # Save remaining results\n",
    "    if new_results:\n",
    "        temp_df = pd.DataFrame(new_results)\n",
    "        df_query_result = pd.concat([df_query_result, temp_df], ignore_index=True)\n",
    "        df_query_result.to_csv(sql_generator_result_path, index=False)\n",
    "\n",
    "    return df_query_result\n",
    "\n",
    "\n",
    "# Initialize directories and configuration\n",
    "inference_result_dir = \"sql_generator_result\"\n",
    "model_name = OPENAI_MODEL\n",
    "database_type = \"core\"\n",
    "current_date = datetime.now().strftime(\"%d %B %Y\")\n",
    "\n",
    "if database_type == \"core\":\n",
    "    schema = employee_schema\n",
    "    relations = employee_relations\n",
    "    master_data = employee_master_data\n",
    "    data_trustee_tables = data_trustee_employee\n",
    "    master_data = employee_master_data\n",
    "else:\n",
    "    schema = time_management_schema\n",
    "    relations = time_management_relations\n",
    "    master_data = time_management_master_data\n",
    "    data_trustee_tables = data_trustee_time_management\n",
    "    master_data = time_management_master_data\n",
    "\n",
    "context_data = {\n",
    "    \"schema\": schema,\n",
    "    \"relations\": relations,\n",
    "    \"master_data\": master_data,\n",
    "    \"data_trustee_tables\": data_trustee_tables,\n",
    "    \"anonymized_entities_description\": anonymized_entities_description,\n",
    "    \"current_date\": current_date\n",
    "}\n",
    "\n",
    "# Test a single query first\n",
    "prompt_builder = OpenAIPromptBuilder(system_prompt)\n",
    "lm_request_processor = LMRequestProcessor(prompt_builder, lm_invoker)\n",
    "text_to_sql = OneToOneQueryTransformer(lm_request_processor)\n",
    "\n",
    "test_query = \"Bagaimana perbandingan jumlah karyawan berdasarkan organisasi? Munculkan nama organisasi dan total karyawan.\"\n",
    "test_result = await text_to_sql.transform({**context_data, \"user_instruction\": test_query})\n",
    "print(\"Test query result:\")\n",
    "print(format_sql_query(test_result[0]))\n",
    "\n",
    "# Process all queries\n",
    "df_query_result = await process_sql_queries(\n",
    "    df_data_test=df_data_test,\n",
    "    inference_result_dir=inference_result_dir,\n",
    "    model_name=model_name,\n",
    "    system_prompt=system_prompt,\n",
    "    lm_invoker=lm_invoker,\n",
    "    context_data=context_data\n",
    ")\n",
    "\n",
    "print(f\"SQL generation complete. Results saved to {os.path.join(inference_result_dir, f'{model_name}-using-catapa-prompt.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]2025-02-28 13:34:23,812 - INFO - Successfully wrote row 1/30\n",
      "2025-02-28 13:34:26,475 - INFO - Successfully wrote row 2/30\n",
      "2025-02-28 13:34:29,170 - INFO - Successfully wrote row 3/30\n",
      "2025-02-28 13:34:31,799 - INFO - Successfully wrote row 4/30\n",
      "2025-02-28 13:34:34,279 - INFO - Successfully wrote row 5/30\n",
      "2025-02-28 13:34:36,714 - INFO - Successfully wrote row 6/30\n",
      "2025-02-28 13:34:39,173 - INFO - Successfully wrote row 7/30\n",
      "2025-02-28 13:34:41,834 - INFO - Successfully wrote row 8/30\n",
      "2025-02-28 13:34:44,599 - INFO - Successfully wrote row 9/30\n",
      "2025-02-28 13:34:47,363 - INFO - Successfully wrote row 10/30\n",
      " 33%|███▎      | 1/3 [00:28<00:57, 28.59s/it]2025-02-28 13:34:51,884 - INFO - Successfully wrote row 11/30\n",
      "2025-02-28 13:34:54,327 - INFO - Successfully wrote row 12/30\n",
      "2025-02-28 13:34:56,797 - INFO - Successfully wrote row 13/30\n",
      "2025-02-28 13:34:59,509 - INFO - Successfully wrote row 14/30\n",
      "2025-02-28 13:35:02,004 - INFO - Successfully wrote row 15/30\n",
      "2025-02-28 13:35:04,464 - INFO - Successfully wrote row 16/30\n",
      "2025-02-28 13:35:07,029 - INFO - Successfully wrote row 17/30\n",
      "2025-02-28 13:35:09,614 - INFO - Successfully wrote row 18/30\n",
      "2025-02-28 13:35:12,144 - INFO - Successfully wrote row 19/30\n",
      "2025-02-28 13:35:14,397 - INFO - Successfully wrote row 20/30\n",
      " 67%|██████▋   | 2/3 [00:55<00:27, 27.68s/it]2025-02-28 13:35:18,020 - WARNING - Rate limit hit. Waiting 2.62 seconds before retry 1\n",
      "2025-02-28 13:35:23,202 - INFO - Successfully wrote row 21/30\n",
      "2025-02-28 13:35:25,662 - INFO - Successfully wrote row 22/30\n",
      "2025-02-28 13:35:28,001 - INFO - Successfully wrote row 23/30\n",
      "2025-02-28 13:35:30,380 - INFO - Successfully wrote row 24/30\n",
      "2025-02-28 13:35:32,932 - INFO - Successfully wrote row 25/30\n",
      "2025-02-28 13:35:35,312 - INFO - Successfully wrote row 26/30\n",
      "2025-02-28 13:35:37,776 - INFO - Successfully wrote row 27/30\n",
      "2025-02-28 13:35:40,076 - INFO - Successfully wrote row 28/30\n",
      "2025-02-28 13:35:42,600 - INFO - Successfully wrote row 29/30\n",
      "2025-02-28 13:35:45,118 - INFO - Successfully wrote row 30/30\n",
      "100%|██████████| 3/3 [01:24<00:00, 28.12s/it]\n",
      "2025-02-28 13:35:45,120 - INFO - Successfully wrote 30 rows\n"
     ]
    }
   ],
   "source": [
    "from modules.google_sheets_writer import GoogleSheetsWriter\n",
    "import logging\n",
    "\n",
    "generate_sql_query = \"generate_sql_query_open_ai\"\n",
    "\n",
    "writer = GoogleSheetsWriter(\n",
    "    google_util=google,  # Your GoogleUtil instance\n",
    "    sheet_id=GOOGLE_SPREADSHEET_ID,\n",
    "    worksheet_name=generate_sql_query,\n",
    "    batch_size=10,  # Customize batch size\n",
    "    max_retries=5,  # Customize retry attempts\n",
    "    batch_delay=2  # Customize delay between batches\n",
    ")\n",
    "# Write the DataFrame\n",
    "result = writer.write_dataframe(df_query_result)\n",
    "\n",
    "# Log results\n",
    "logging.info(f\"Successfully wrote {result.successful_rows} rows\")\n",
    "if result.failed_rows > 0:\n",
    "    logging.error(f\"Failed to write {result.failed_rows} rows\")\n",
    "    for error in result.errors:\n",
    "        logging.error(f\"Row {error['row_number']}: {error['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txt-to-sql-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
