{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt\n",
    "# !pip install -q tqdm\n",
    "# !pip install transformers==4.49.0\n",
    "# !pip install python-dotenv\n",
    "# !pip install google-api-python-client==2.100.0 gspread==5.10.0\n",
    "# !pip install huggingface-hub==0.29.1\n",
    "# !pip install sqlglot\n",
    "# !pip install langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model into local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/myenv-text-to-sql/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Download model into local\n",
    "import getpass\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import login\n",
    "HF_TOKEN = getpass.getpass(\"Enter your Hugging Face token: \")\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-cli download mistralai/Mistral-7B-Instruct-v0.2 --revision main --local-dir text_to_sql/models/mistralai/Mistral-7B-Instruct-v0.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the model repository name (change this to the desired model)\n",
    "model_repo = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# branch = \"refs/pr/1\"\n",
    "branch = \"main\"\n",
    "# Define the directory where the model will be saved\n",
    "save_directory = os.path.join(\"text_to_sql/models\", model_repo)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Use the Hugging Face CLI to download the model\n",
    "download_cmd = f\"huggingface-cli download {model_repo} --revision {branch} --local-dir {save_directory}\"\n",
    "# os.system(download_cmd)\n",
    "# prompt = \"huggingface-cli download Ellbendls/Qwen-2.5-3b-Text_to_SQL --local-dir models/Ellbendls/Qwen-2.5-3b-Text_to_SQL\"\n",
    "# print(f\"Model downloaded to {save_directory}\")\n",
    "download_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_SPREADSHEET_ID: str = \"1dDMqrol_DrEMjvLy88IRu2WdHN7T5BU0LrD8ORLuNPI\" # put your spreadsheet id here\n",
    "GOOGLE_SPREADSHEET_URL: str = f\"https://docs.google.com/spreadsheets/d/{GOOGLE_SPREADSHEET_ID}/edit?usp=sharing\" # put your spreadsheet link here\n",
    "DATA_TEST_SHEET_NAME: str = \"test_data\"\n",
    "\n",
    "GOOGLE_SHEETS_CLIENT_EMAIL: str = os.getenv('GOOGLE_SHEETS_CLIENT_EMAIL')\n",
    "GOOGLE_SHEETS_PRIVATE_KEY: str = os.getenv('GOOGLE_SHEETS_PRIVATE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Google Auth\n",
    "# Google Authentication\n",
    "from modules.google_sheets_writer import GoogleUtil\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "PRIVATE_KEY = GOOGLE_SHEETS_PRIVATE_KEY\n",
    "google: GoogleUtil = GoogleUtil(PRIVATE_KEY, GOOGLE_SHEETS_CLIENT_EMAIL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Id</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Expected SQL Query</th>\n",
       "      <th>Expected Query Result</th>\n",
       "      <th>Sheet</th>\n",
       "      <th>Database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Bagaimana perbandingan jumlah karyawan berdasa...</td>\n",
       "      <td>SELECT organizations.name AS \"organization_nam...</td>\n",
       "      <td>[{'organization_name': 'Information Technology...</td>\n",
       "      <td>catapa_test_core_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bagaimana data termination berdasarkan nama ja...</td>\n",
       "      <td>SELECT job_titles.name AS \"job_title_name\", CO...</td>\n",
       "      <td>[{'job_title_name': 'Information Technology', ...</td>\n",
       "      <td>catapa_test_core_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No Id                                             Prompt  \\\n",
       "0  1  1  Bagaimana perbandingan jumlah karyawan berdasa...   \n",
       "1  2  2  Bagaimana data termination berdasarkan nama ja...   \n",
       "\n",
       "                                  Expected SQL Query  \\\n",
       "0  SELECT organizations.name AS \"organization_nam...   \n",
       "1  SELECT job_titles.name AS \"job_title_name\", CO...   \n",
       "\n",
       "                               Expected Query Result  \\\n",
       "0  [{'organization_name': 'Information Technology...   \n",
       "1  [{'job_title_name': 'Information Technology', ...   \n",
       "\n",
       "                       Sheet Database  \n",
       "0  catapa_test_core_employee     core  \n",
       "1  catapa_test_core_employee     core  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Load Data Test\n",
    "rows: List[list] = google.retrieve_worksheet(GOOGLE_SPREADSHEET_ID, DATA_TEST_SHEET_NAME)\n",
    "df_data_test: pd.DataFrame = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "display(df_data_test.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/text_to_sql_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.66it/s]\n",
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_1830/3968238050.py:31: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  hf_pipeline = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "import torch\n",
    "model_dir = \"unsloth\"  # based on your base model path.\n",
    "model_name = \"Qwen2.5-Coder-7B-Instruct\" # based on your base model path.\n",
    "model_id = os.path.join(model_dir, model_name)\n",
    "sft_path = \"/home/text_to_sql/fine_tuned/fine_tune_exp_1/unsloth/Qwen2.5-Coder-7B-Instruct/exp_id_1:fine_tuning_ft-1:ft_text_to_sql_few_shot_1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, sft_path)  # uncomment if you have fine tuned model\n",
    "model = model.merge_and_unload()  # uncomment if you have fine tuned model\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=base_model,\n",
    "    device=0,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    tokenizer=tokenizer,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    return_full_text=False,\n",
    "    model_kwargs = {\"temperature\": 0, \"do_sample\":True},\n",
    ")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a SQL generator expert for MariaDB 10.5.23. Your task is to create SQL queries based on database schema, table relationships, master data, current date, and user instructions.\n",
    "\n",
    "1. CORE SQL REQUIREMENTS:\n",
    "   - Generate directly executable MariaDB 10.5.23 SQL\n",
    "   - DO NOT select identifiers (id, employee_id) except within aggregate functions (MAX, SUM, AVG)\n",
    "   - Always use human-readable names (religions.name instead of religions.id)\n",
    "   - Prefix all column names with table names to avoid ambiguity\n",
    "   - Use snake_case for column aliases in SELECT clause only (no aliases in FROM)\n",
    "   - Handle dates properly:\n",
    "     * Wrap all date literals in STR_TO_DATE()\n",
    "     * Ensure date comparisons use CAST() for DATE type\n",
    "   - Use aggregate functions ONLY in SELECT or HAVING clauses (never in WHERE)\n",
    "   - Ensure JOIN conditions reference correct foreign keys\n",
    "   - Add extra JOINs to master tables if display names are missing\n",
    "   - Handle division by checking for non-zero denominators\n",
    "   - Name output columns based on user's instruction language\n",
    "\n",
    "2. DATA TRUSTEE REQUIREMENTS:\n",
    "   - If any table in the query appears in the Data Trustee Enabled Tables list:\n",
    "     * JOIN with the employment_statuses table using employee_id column\n",
    "     * Add these exact filters:\n",
    "       employment_statuses.organization_id IN ('[ORGANIZATION_IDS]')\n",
    "       AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]')\n",
    "       AND employment_statuses.location_id IN ('[LOCATION_IDS]')\n",
    "     * If table requires prerequisite join (indicated in data_trustee_tables),\n",
    "       perform that JOIN before joining with employment_statuses\n",
    "   - If no table from the Data Trustee Enabled Tables is used, do not include\n",
    "     any data trustee-specific JOINs or filters\n",
    "\n",
    "3. OUTPUT FORMAT:\n",
    "   - Return ONLY a JSON object with your SQL query in this exact format:\n",
    "        {{\n",
    "        \"sql_query\": \"<your_generated_sql_query>\"\n",
    "        }}\n",
    "   - No explanations or commentary\n",
    "\n",
    "Examples:\n",
    "Input: \"Berapa total uang yang ditransfer untuk karyawan yang aktif di bulan Oktober 2023?\"\n",
    "\n",
    "Output: {\n",
    "  \"sql_query\": \"SELECT SUM(salary_payment_summaries.transferred_amount) AS 'total_transferred_amount' FROM salary_payment_summaries JOIN salary_payments ON salary_payment_summaries.id = salary_payments.salary_payment_summary_id JOIN employees ON salary_payments.employee_id = employees.id JOIN employment_statuses ON employees.id = employment_statuses.employee_id WHERE employees.active = TRUE AND employment_statuses.organization_id IN ([ORGANIZATION_IDS]) AND employment_statuses.job_level_id IN ([JOB_LEVEL_IDS]) AND employment_statuses.location_id IN ([LOCATION_IDS]) AND salary_payment_summaries.payment_date BETWEEN STR_TO_DATE('2023-10-01', '%Y-%m-%d') AND STR_TO_DATE('2023-10-31', '%Y-%m-%d');\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Generate a SQL query for the following instruction:\n",
    "\n",
    "DATABASE INFORMATION:\n",
    "Schema: {schema}\n",
    "Relationships: {relations}\n",
    "Master Data: {master_data}\n",
    "Data Trustee Enabled Tables: {data_trustee_tables}\n",
    "Anonymized Entities: {anonymized_entities_description}\n",
    "Current Date: {current_date}\n",
    "\n",
    "user_instruction: {user_instruction}\n",
    "Return only the SQL query as a JSON object: {{\"sql_query\": \"<your_sql_query>\"}}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL queries:   0%|          | 0/102 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 1 because it already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL queries:   2%|▏         | 2/102 [00:03<02:32,  1.53s/it]"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from modules.database_info.schema import employee_schema, time_management_schema\n",
    "from modules.database_info.master_data import employee_master_data, time_management_master_data\n",
    "from modules.database_info.relation import employee_relations, time_management_relations\n",
    "from modules.database_info.trustee_tables import data_trustee_employee, data_trustee_time_management\n",
    "from modules.database_info.anonymize_entities import anonymized_entities_description\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "inference_result_dir = \"sql_generator_result\"\n",
    "os.makedirs(inference_result_dir, exist_ok=True)\n",
    "current_date = datetime.now().strftime(\"%d %B %Y\")\n",
    "sql_generator_result_path = os.path.join(inference_result_dir, f\"{model_name}-using-catapa-prompt.csv\")\n",
    "\n",
    "\n",
    "for index, df_row in tqdm(enumerate(df_data_test.iterrows()), desc=\"Generating SQL queries\", total=len(df_data_test)):\n",
    "    user_instruction = df_row[1]['Prompt']\n",
    "    no = df_row[1]['No']\n",
    "    if \"Id\" in df_row[1]:\n",
    "        id = df_row[1]['Id']\n",
    "    else:\n",
    "        id = \"-\"\n",
    "    database_type = df_row[1]['Database']\n",
    "    if not os.path.exists(sql_generator_result_path):\n",
    "        df_query_result = pd.DataFrame(\n",
    "            columns=[\n",
    "                'No',\n",
    "                'Id',\n",
    "                'Prompt',\n",
    "                'Generated SQL Query',\n",
    "                'Expected SQL Query',\n",
    "                'Expected Query Result',\n",
    "                'Database',\n",
    "                'Time Taken'\n",
    "            ]\n",
    "        )\n",
    "        df_query_result.to_csv(sql_generator_result_path, index=False)\n",
    "    else:\n",
    "        df_query_result = pd.read_csv(sql_generator_result_path)\n",
    "\n",
    "    if int(no) in df_query_result['No'].to_list():\n",
    "        print(f\"Skipping {no} because it already exists\")\n",
    "        continue\n",
    "    # if index > 0:  # 15, 25\n",
    "    #     continue\n",
    "\n",
    "    start_time = time()\n",
    "    if database_type == \"core\":\n",
    "        schema = employee_schema\n",
    "        relations = employee_relations\n",
    "        master_data = employee_master_data\n",
    "        data_trustee_tables = data_trustee_employee\n",
    "        master_data = employee_master_data\n",
    "    else:\n",
    "        schema = time_management_schema\n",
    "        relations = time_management_relations\n",
    "        master_data = time_management_master_data\n",
    "        data_trustee_tables = data_trustee_time_management\n",
    "        master_data = time_management_master_data\n",
    "\n",
    "    formatted_user_prompt = USER_PROMPT.format(\n",
    "        schema=schema,\n",
    "        relations=relations,\n",
    "        master_data=master_data,\n",
    "        data_trustee_tables=data_trustee_tables,\n",
    "        anonymized_entities_description=anonymized_entities_description,\n",
    "        current_date=current_date,\n",
    "        user_instruction=user_instruction,\n",
    "        query_result = \"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": formatted_user_prompt},\n",
    "    ]\n",
    "    # Check if the tokenizer has a chat template\n",
    "    has_chat_template = hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None\n",
    "\n",
    "    if has_chat_template:\n",
    "        # Tokenize input with chat template\n",
    "        inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        inputs = messages\n",
    "\n",
    "    sql_generator_result = hf_pipeline.invoke(inputs)\n",
    "    # print(sql_generator_result)\n",
    "    time_taken = time() - start_time\n",
    "    # Create DataFrame with explicit index to avoid ValueError\n",
    "    df_query_generator = pd.DataFrame({\n",
    "        'No': [no],  # Wrap scalar values in lists\n",
    "        'Id': [id],\n",
    "        'Prompt': [user_instruction],\n",
    "        'Generated SQL Query': [sql_generator_result],\n",
    "        'Expected SQL Query': [df_row[1]['Expected SQL Query']],\n",
    "        'Expected Query Result': [df_row[1]['Expected Query Result']],\n",
    "        'Database': [database_type],\n",
    "        'Time Taken': [time_taken]\n",
    "    })\n",
    "    # Concatenate with existing results\n",
    "    df_query_result = pd.concat([df_query_result, df_query_generator], ignore_index=True)\n",
    "    df_query_result.to_csv(sql_generator_result_path, index=False)\n",
    "display(df_query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sql_generator_result/CodeLlama-7b-Instruct-hf-using-catapa-prompt.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_generator_result_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.google_sheets_writer import GoogleSheetsWriter\n",
    "import logging\n",
    "\n",
    "INFERENCE_RESULT_WORKSHEET = \"experiment_inference_result\"\n",
    "\n",
    "writer = GoogleSheetsWriter(\n",
    "    google_util=google,  # Your GoogleUtil instance\n",
    "    sheet_id=GOOGLE_SPREADSHEET_ID,\n",
    "    worksheet_name=\"experiment_inference_result\",\n",
    "    batch_size=10,  # Customize batch size\n",
    "    max_retries=5,  # Customize retry attempts\n",
    "    batch_delay=2  # Customize delay between batches\n",
    ")\n",
    "# Write the DataFrame\n",
    "result = writer.write_dataframe(df_query_result)\n",
    "\n",
    "# Log results\n",
    "logging.info(f\"Successfully wrote {result.successful_rows} rows\")\n",
    "if result.failed_rows > 0:\n",
    "    logging.error(f\"Failed to write {result.failed_rows} rows\")\n",
    "    for error in result.errors:\n",
    "        logging.error(f\"Row {error['row_number']}: {error['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
