{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA question generator using Deepseek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**\n",
    "1. Alfan Dinda Rahmawan (alfan.d.rahmawan@gdplabs.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting google-api-python-client==2.100.0\n",
      "  Downloading google_api_python_client-2.100.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting gspread==5.10.0\n",
      "  Downloading gspread-5.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httplib2<1.dev0,>=0.15.0 (from google-api-python-client==2.100.0)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth<3.0.0.dev0,>=1.19.0 (from google-api-python-client==2.100.0)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-auth-httplib2>=0.1.0 (from google-api-python-client==2.100.0)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client==2.100.0)\n",
      "  Downloading google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client==2.100.0)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting google-auth-oauthlib>=0.4.1 (from gspread==5.10.0)\n",
      "  Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.100.0)\n",
      "  Downloading googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.100.0)\n",
      "  Downloading protobuf-6.30.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.100.0)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in ./.venv/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.100.0) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.100.0)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.100.0)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.100.0)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib>=0.4.1->gspread==5.10.0)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.15.0->google-api-python-client==2.100.0)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client==2.100.0)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.100.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.100.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.100.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client==2.100.0) (2025.1.31)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread==5.10.0)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading google_api_python_client-2.100.0-py2.py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gspread-5.10.0-py3-none-any.whl (44 kB)\n",
      "Downloading google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.30.1-cp39-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, protobuf, oauthlib, cachetools, rsa, requests-oauthlib, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, google-auth, google-auth-oauthlib, google-auth-httplib2, google-api-core, gspread, google-api-python-client\n",
      "Successfully installed cachetools-5.5.2 google-api-core-2.24.2 google-api-python-client-2.100.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-auth-oauthlib-1.2.1 googleapis-common-protos-1.69.2 gspread-5.10.0 httplib2-0.22.0 oauthlib-3.2.2 proto-plus-1.26.1 protobuf-6.30.1 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyparsing-3.2.3 requests-oauthlib-2.0.0 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain==\"0.3.0\"\n",
    "%pip install -q langchain-aws==\"0.2.7\"\n",
    "%pip install -q langchain_openai==\"0.2.14\"\n",
    "%pip install -q boto3==\"1.35.71\"\n",
    "%pip install -q pandas==\"2.2.2\"\n",
    "%pip install -q tqdm==\"4.66.4\"\n",
    "%pip install google-api-python-client==2.100.0 gspread==5.10.0\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_SPREADSHEET_ID: str = \"1dDMqrol_DrEMjvLy88IRu2WdHN7T5BU0LrD8ORLuNPI\" # put your spreadsheet id here\n",
    "GOOGLE_SPREADSHEET_URL: str = f\"https://docs.google.com/spreadsheets/d/{GOOGLE_SPREADSHEET_ID}/edit?usp=sharing\" # put your spreadsheet link here\n",
    "DATA_TEST_SHEET_NAME: str = \"catapa_syntetics_question\"\n",
    "\n",
    "GOOGLE_SHEETS_CLIENT_EMAIL: str = os.getenv('GOOGLE_SHEETS_CLIENT_EMAIL')\n",
    "GOOGLE_SHEETS_PRIVATE_KEY: str = os.getenv('GOOGLE_SHEETS_PRIVATE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Authentication\n",
    "from modules.google_sheets_writer import GoogleUtil\n",
    "\n",
    "PRIVATE_KEY = GOOGLE_SHEETS_PRIVATE_KEY\n",
    "google: GoogleUtil = GoogleUtil(PRIVATE_KEY, GOOGLE_SHEETS_CLIENT_EMAIL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "employee_schema_path = \"schema/employee_table.json\"\n",
    "payroll_schema_path = \"schema/payroll_table.json\"\n",
    "time_management_schema_path = \"schema/time_management_table.json\"\n",
    "\n",
    "master_data_attendance_path = \"master_data/attendance_statuses.csv\"\n",
    "master_data_employment_status_path = \"master_data/employment_status_types.csv\"\n",
    "master_data_employment_path = \"master_data/employment_types.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_column_info(column: dict) -> str:\n",
    "    \"\"\"Format column information with type and description.\n",
    "\n",
    "    Args:\n",
    "        column (dict): Dictionary containing column information with at least a 'name' key.\n",
    "            May also contain 'type' and 'description' keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted column information string.\n",
    "    \"\"\"\n",
    "    parts = [column['name']]\n",
    "\n",
    "    # Add type if present\n",
    "    if 'type' in column:\n",
    "        parts.append(f\"[{column['type']}]\")\n",
    "\n",
    "    # Add description if present\n",
    "    if 'description' in column:\n",
    "        parts.append(f\"({column['description']})\")\n",
    "\n",
    "    return \" \".join(parts)\n",
    "\n",
    "def create_schema_dictionary(schema_data: dict) -> str:\n",
    "    \"\"\"Create formatted schema information for all tables.\n",
    "\n",
    "    Args:\n",
    "        schema_data (dict): Dictionary containing table schema information.\n",
    "            Each key is a table name, and each value is a dictionary with table information.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted schema information for all tables.\n",
    "    \"\"\"\n",
    "    formatted_schemas = []\n",
    "\n",
    "    for table_name, table_info in schema_data.items():\n",
    "        # Get columns with a default empty list if not present\n",
    "        columns = table_info.get('columns', [])\n",
    "\n",
    "        # Format column information using list comprehension\n",
    "        column_info = [f\"- {format_column_info(column)}\" for column in columns]\n",
    "\n",
    "        # Join column information with newlines\n",
    "        formatted_schemas.append(f\"{table_name}:\\n\" + \"\\n\".join(column_info))\n",
    "\n",
    "    return \"\\n\\n\".join(formatted_schemas)\n",
    "\n",
    "# Initialize the main dictionary with three domains\n",
    "table_schema = {domain: \"\" for domain in ['employee', 'payroll', 'time_management']}\n",
    "\n",
    "# Map files to their domains\n",
    "file_domain_mapping = {\n",
    "    employee_schema_path: 'employee',\n",
    "    payroll_schema_path: 'payroll',\n",
    "    time_management_schema_path: 'time_management'\n",
    "}\n",
    "\n",
    "# Process each schema file\n",
    "for schema_file, domain in file_domain_mapping.items():\n",
    "    try:\n",
    "        with open(schema_file) as f:\n",
    "            schema_data = json.load(f)\n",
    "            table_schema[domain] = create_schema_dictionary(schema_data)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing schema file {schema_file}: {e}\")\n",
    "\n",
    "## Show Schema\n",
    "# print(table_schema['employee'])\n",
    "# print(table_schema['payroll'])\n",
    "# print(table_schema['time_management'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_relations_dictionary(schema_data: dict) -> str:\n",
    "    \"\"\"Create formatted relations information for all tables.\n",
    "\n",
    "    Args:\n",
    "        schema_data (dict): Dictionary containing table schema information.\n",
    "            Each key is a table name, and each value is a dictionary with table information.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted relations information for all tables.\n",
    "    \"\"\"\n",
    "    formatted_relations = []\n",
    "\n",
    "    for table_name, table_info in schema_data.items():\n",
    "        # Skip if no foreign keys\n",
    "        if 'foreign_keys' not in table_info or not table_info['foreign_keys']:\n",
    "            continue\n",
    "\n",
    "        # Start with table name\n",
    "        relations = [f\"{table_name}:\"]\n",
    "\n",
    "        # Add each foreign key relation\n",
    "        relations.extend([\n",
    "            f\"- {fk['ref_table']} referenced by {fk['column']}\"\n",
    "            for fk in table_info['foreign_keys']\n",
    "        ])\n",
    "\n",
    "        formatted_relations.append(\"\\n\".join(relations))\n",
    "\n",
    "    return \"\\n\".join(formatted_relations)\n",
    "\n",
    "# Initialize the relations dictionary for all domains\n",
    "domains = ['employee', 'payroll', 'time_management']\n",
    "table_relations = {domain: \"\" for domain in domains}\n",
    "\n",
    "# Map schema files to their domains\n",
    "file_domain_mapping = {\n",
    "    employee_schema_path: 'employee',\n",
    "    payroll_schema_path: 'payroll',\n",
    "    time_management_schema_path: 'time_management'\n",
    "}\n",
    "\n",
    "# Process each schema file\n",
    "for schema_file, domain in file_domain_mapping.items():\n",
    "    try:\n",
    "        with open(schema_file) as f:\n",
    "            schema_data = json.load(f)\n",
    "            table_relations[domain] = create_relations_dictionary(schema_data)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error processing relations in {schema_file}: {e}\")\n",
    "\n",
    "# show relations\n",
    "# print(table_relations['employee'])\n",
    "# print(table_relations['payroll'])\n",
    "# print(table_relations['time_management'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Master Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "class MasterDataLoader:\n",
    "    \"\"\"Class to load and process master data from CSV files and schema files.\n",
    "\n",
    "    This class handles loading master data from CSV files and extracting master data\n",
    "    information from schema files, with proper error handling and logging.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, master_data_paths: Dict[str, str], schema_paths: Dict[str, str]):\n",
    "        \"\"\"Initialize the MasterDataLoader with paths to master data and schema files.\n",
    "\n",
    "        Args:\n",
    "            master_data_paths: Dictionary mapping data types to file paths\n",
    "            schema_paths: Dictionary mapping domains to schema file paths\n",
    "        \"\"\"\n",
    "        self.master_data_paths = master_data_paths\n",
    "        self.schema_paths = schema_paths\n",
    "        self.master_data_dict = {}\n",
    "        self.db_master_data_dict = {domain: \"\" for domain in schema_paths.keys()}\n",
    "\n",
    "    def read_csv_master_data(self, file_path: str) -> Optional[List[str]]:\n",
    "        \"\"\"Read CSV file and return list of values from 'name' column.\n",
    "\n",
    "        Args:\n",
    "            file_path: Path to the CSV file containing master data\n",
    "\n",
    "        Returns:\n",
    "            List of values from the 'name' column or None if file not found or invalid\n",
    "        \"\"\"\n",
    "        path = Path(file_path)\n",
    "        if not path.exists():\n",
    "            print(f\"Warning: Master data file not found: {file_path}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                csv_reader = csv.DictReader(f)\n",
    "                # Verify 'name' column exists\n",
    "                if 'name' not in csv_reader.fieldnames:\n",
    "                    print(f\"Warning: CSV file {file_path} does not contain a 'name' column\")\n",
    "                    return None\n",
    "                return [row['name'] for row in csv_reader]\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV file {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_schema_master_data(schema_data: Dict[str, Any]) -> str:\n",
    "        \"\"\"Extract master_data information from schema.\n",
    "\n",
    "        Args:\n",
    "            schema_data: Dictionary containing schema data\n",
    "\n",
    "        Returns:\n",
    "            Formatted string of master data information\n",
    "        \"\"\"\n",
    "        master_data_info = []\n",
    "\n",
    "        for table_name, table_info in schema_data.items():\n",
    "            if isinstance(table_info, dict) and 'master_data' in table_info:\n",
    "                for field, values in table_info['master_data'].items():\n",
    "                    master_data_info.append(f\"{table_name} - {field} = {values}\")\n",
    "\n",
    "        return \"\\n\".join(master_data_info)\n",
    "\n",
    "    def load_master_data(self) -> Dict[str, str]:\n",
    "        \"\"\"Load master data from CSV files.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping data types to formatted master data strings\n",
    "        \"\"\"\n",
    "        for key, file_path in self.master_data_paths.items():\n",
    "            values = self.read_csv_master_data(file_path)\n",
    "            if values:  # Only add if we got values\n",
    "                self.master_data_dict[key] = f\"{key} - name = {values}\"\n",
    "\n",
    "        return self.master_data_dict\n",
    "\n",
    "    def load_schema_master_data(self) -> Dict[str, str]:\n",
    "        \"\"\"Load master data from schema files.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping domains to formatted master data strings\n",
    "        \"\"\"\n",
    "        for schema_file, domain in self.schema_paths.items():\n",
    "            path = Path(schema_file)\n",
    "            if not path.exists():\n",
    "                print(f\"Warning: Schema file not found: {schema_file}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                with path.open('r', encoding='utf-8') as f:\n",
    "                    try:\n",
    "                        schema_data = json.load(f)\n",
    "                        self.db_master_data_dict[domain] = self.extract_schema_master_data(schema_data)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: Invalid JSON in schema file: {schema_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading schema file {schema_file}: {e}\")\n",
    "\n",
    "        return self.db_master_data_dict\n",
    "\n",
    "    def load_all(self) -> tuple[Dict[str, str], Dict[str, str]]:\n",
    "        \"\"\"Load all master data from both CSV and schema files.\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing (master_data_dict, db_master_data_dict)\n",
    "        \"\"\"\n",
    "        self.load_master_data()\n",
    "        self.load_schema_master_data()\n",
    "        return self.master_data_dict, self.db_master_data_dict\n",
    "\n",
    "# Usage example:\n",
    "# Define master data files\n",
    "master_data_files = {\n",
    "    'attendance_statuses': master_data_attendance_path,\n",
    "    'employment_status_types': master_data_employment_status_path,\n",
    "    'employment_types': master_data_employment_path\n",
    "}\n",
    "\n",
    "# Map schema files to their domains\n",
    "schema_file_mapping = {\n",
    "    employee_schema_path: 'employee',\n",
    "    payroll_schema_path: 'payroll',\n",
    "    time_management_schema_path: 'time_management'\n",
    "}\n",
    "\n",
    "# Create loader and load all data\n",
    "loader = MasterDataLoader(master_data_files, schema_file_mapping)\n",
    "master_data_dict, db_master_data_dict = loader.load_all()\n",
    "\n",
    "# Show result\n",
    "# print(\"Master Data Dictionary:\")\n",
    "# print(master_data_dict)\n",
    "# print(\"\\nDB Master Data Dictionary:\")\n",
    "# print(db_master_data_dict.keys())\n",
    "# print(db_master_data_dict['employee'])\n",
    "employee_master_data = master_data_dict['employment_status_types'] + '\\n' + master_data_dict['employment_types'] + '\\n' + db_master_data_dict['employee']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Trustee Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "def extract_data_trustee(schema_data: Dict) -> str:\n",
    "    \"\"\"Extract data_trustee information from schema.\n",
    "\n",
    "    Args:\n",
    "        schema_data: Dictionary containing table schema information\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted string of data trustee information with each entry on a new line\n",
    "    \"\"\"\n",
    "    trustee_info = []\n",
    "\n",
    "    for table_name, table_info in schema_data.items():\n",
    "        if isinstance(table_info, dict) and 'data_trustee' in table_info:\n",
    "            trustee_info.append(f\"> {table_name}: {table_info['data_trustee']}\")\n",
    "\n",
    "    return \"\\n\".join(trustee_info)\n",
    "\n",
    "def load_data_trustee_info(schema_paths: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"Load data trustee information from schema files.\n",
    "\n",
    "    Args:\n",
    "        schema_paths: Dictionary mapping schema file paths to domain names\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Dictionary mapping domains to formatted data trustee information\n",
    "    \"\"\"\n",
    "    # Initialize the data trustee dictionary with empty strings for each domain\n",
    "    data_trustee_dict = {domain: \"\" for domain in set(schema_paths.values())}\n",
    "\n",
    "    # Process each schema file\n",
    "    for schema_file, domain in schema_paths.items():\n",
    "        path = Path(schema_file)\n",
    "        if not path.exists():\n",
    "            print(f\"Warning: Schema file not found: {schema_file}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with path.open('r', encoding='utf-8') as f:\n",
    "                schema_data = json.load(f)\n",
    "                data_trustee_dict[domain] = extract_data_trustee(schema_data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Invalid JSON in schema file: {schema_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing schema file {schema_file}: {e}\")\n",
    "\n",
    "    return data_trustee_dict\n",
    "\n",
    "# Map files to their domains\n",
    "file_domain_mapping = {\n",
    "    employee_schema_path: 'employee',\n",
    "    payroll_schema_path: 'payroll',\n",
    "    time_management_schema_path: 'time_management'\n",
    "}\n",
    "\n",
    "# Load data trustee information\n",
    "data_trustee_dict = load_data_trustee_info(file_domain_mapping)\n",
    "\n",
    "# Example usage:\n",
    "# print(data_trustee_dict['employee'])\n",
    "# print(data_trustee_dict['payroll'])\n",
    "# print(data_trustee_dict['time_management'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymized Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entities:\n",
    "    PERSON = \"PERSON\"\n",
    "    URL = \"URL\"\n",
    "    EMAIL_ADDRESS = \"EMAIL_ADDRESS\"\n",
    "    KTP = \"ID_KTP\"\n",
    "    NPWP = \"ID_NPWP\"\n",
    "    PHONE_NUMBER = \"PHONE_NUMBER\"\n",
    "    FACEBOOK_ACCOUNT = \"FACEBOOK_ACCOUNT\"\n",
    "    FAMILY_CARD_NUMBER = \"FAMILY_CARD_NUMBER\"\n",
    "    BANK_ACCOUNT = \"BANK_ACCOUNT\"\n",
    "\n",
    "class CatapaEntities:\n",
    "    BIRTHDATE = \"BIRTHDATE\"\n",
    "    EMPLOYEE_IDENTIFICATION_NUMBER = \"EMPLOYEE_IDENTIFICATION_NUMBER\"\n",
    "\n",
    "ENTITIES_DESCRIPTION = {\n",
    "    Entities.PERSON: \"Represents an individual human being, identified by a name, which can be a full name or partial \"\n",
    "    \"name (e.g., 'John', 'Doe', 'Maria', 'Gomez'). This entity strictly refers to proper names and excludes job titles,\"\n",
    "    \" roles, or organizational terms (e.g., 'Jumlah Karyawan Baru').\",\n",
    "    Entities.URL: \"A Uniform Resource Locator (URL) is the address used to access a resource on the internet, \"\n",
    "    \"typically pointing to a website or document.\",\n",
    "    Entities.EMAIL_ADDRESS: \"A unique identifier for electronic mail communication, usually in the format \"\n",
    "    \"'user@example.com'.\",\n",
    "    Entities.KTP: \"An Indonesian term for 'Kartu Tanda Penduduk', referring to a National Identity Card issued \"\n",
    "    \"to citizens of Indonesia.\",\n",
    "    Entities.NPWP: \"An Indonesian term for 'Nomor Pokok Wajib Pajak', referring to a Taxpayer Identification \"\n",
    "    \"Number issued by the Indonesian tax authority to individuals and entities for tax reporting purposes.\",\n",
    "    Entities.PHONE_NUMBER: \"A unique sequence of digits assigned to a telecommunications line, typically used for \"\n",
    "    \"voice or text communication, and often associated with an individual or business for contact purposes.\",\n",
    "    Entities.FACEBOOK_ACCOUNT: \"A social media account associated with Facebook, typically identified by a username \"\n",
    "    \"or profile URL, and used for communication, social networking, and sharing content.\",\n",
    "    Entities.FAMILY_CARD_NUMBER: \"A unique identification number assigned to an Indonesian family, typically \"\n",
    "    \"consisting of 16 digits. It is associated with the Family Card (Kartu Keluarga), which contains information \"\n",
    "    \"about the family members and is used for various administrative and legal purposes in Indonesia.\",\n",
    "    Entities.BANK_ACCOUNT: \"A unique identifier assigned to a bank account, used to facilitate financial transactions\"\n",
    "    \" such as deposits, withdrawals, and transfers.\",\n",
    "    CatapaEntities.BIRTHDATE: \"A date representing the birthdate of an individual\",\n",
    "    CatapaEntities.EMPLOYEE_IDENTIFICATION_NUMBER: \"A unique identifier assigned to an employee within an organization.\"\n",
    "    \" The term 'identification number' is commonly used\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_entities_description(entities_dict: dict) -> str:\n",
    "    \"\"\"Format entities description dictionary into a JSON-like string format.\n",
    "\n",
    "    This function takes a dictionary of entity descriptions and formats it into a\n",
    "    properly indented, JSON-like string with escaped quotes for use in prompts.\n",
    "\n",
    "    Args:\n",
    "        entities_dict: Dictionary mapping entity names to their descriptions\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted string representation of the entities dictionary\n",
    "    \"\"\"\n",
    "    # Create list of formatted key-value pairs\n",
    "    formatted_entries = []\n",
    "\n",
    "    for key, value in entities_dict.items():\n",
    "        # Escape any double quotes in the description\n",
    "        escaped_value = value.replace('\"', '\\\\\"')\n",
    "        # Format the key-value pair with proper JSON syntax\n",
    "        formatted_entries.append(f'\"{key}\": \"{escaped_value}\"')\n",
    "\n",
    "    # Join all entries with proper formatting (comma and newline)\n",
    "    joined_entries = \",\\n    \".join(formatted_entries)\n",
    "\n",
    "    # Add consistent indentation for the entire block\n",
    "    return \"    \" + joined_entries\n",
    "\n",
    "# Generate the formatted string\n",
    "anonymized_entities_description = format_entities_description(ENTITIES_DESCRIPTION)\n",
    "\n",
    "# Print the result\n",
    "# print(anonymized_entities_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sets up the qa generator prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt question generator with sql query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "from modules.database_info.schema import employee_schema, time_management_schema\n",
    "from modules.database_info.master_data import employee_master_data, time_management_master_data\n",
    "from modules.database_info.relation import employee_relations, time_management_relations\n",
    "from modules.database_info.trustee_tables import data_trustee_employee, data_trustee_time_management\n",
    "from modules.database_info.anonymize_entities import anonymized_entities_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE=  \"\"\"<instructions>\n",
    "You are a specialized SQL question generator for an HR database. Your task is to create diverse, realistic business questions AND their corresponding SQL queries against the provided HR database schema.\n",
    "\n",
    "ADDITIONAL GUIDELINES\n",
    "- Ensure questions are realistic for HR analysis scenarios\n",
    "- Be specific about time periods, criteria, and expected outputs\n",
    "- Consider privacy concerns with data trustee fields and anonymized entities\n",
    "- Create questions that would be valuable for HR decision-making\n",
    "- Ensure overall coverage of all major tables and SQL concepts\n",
    "- Make sure the output is in valid, properly formatted JSON\n",
    "- SQL queries should be correct, optimized, and follow best practices\n",
    "- Include comments in complex SQL queries to explain the logic\n",
    "</instructions>\"\"\"\n",
    "\n",
    "USER_MESSAGE = \"\"\"I need you to generate {total_questions} SQL business questions along with their corresponding SQL queries based on this HR database information:\n",
    "\n",
    "    ## DATABASE SCHEMA\n",
    "    {schema}\n",
    "\n",
    "    ## DATABASE RELATIONS\n",
    "    {relations}\n",
    "\n",
    "    ## MASTER DATA\n",
    "    {master_data}\n",
    "\n",
    "    ## DATA TRUSTEE (Sensitive Fields)\n",
    "    {data_trustee_tables}\n",
    "\n",
    "    ## ANONYMIZED ENTITIES\n",
    "    {anonymized_entities_description}\n",
    "\n",
    "\n",
    "    ## SPECIFIC HR ANALYSIS CASES TO INCLUDE\n",
    "    Your questions should cover these specific HR analysis cases, in addition to others you create:\n",
    "\n",
    "    1. TURNOVER ANALYSIS:\n",
    "    - Annual turnover rates over multiple years\n",
    "    - Turnover by department, job level, or location\n",
    "    - Reasons for termination analysis\n",
    "\n",
    "    2. EMPLOYEE HIRING PATTERNS:\n",
    "    - Quarterly new employee count\n",
    "    - Hiring trends by department or location\n",
    "    - Seasonal hiring patterns\n",
    "\n",
    "    3. MANAGEMENT HIERARCHY:\n",
    "    - Employee managerial status (Manager vs. Non-Manager)\n",
    "    - Reporting structure analysis\n",
    "    - Span of control (number of direct reports per manager)\n",
    "\n",
    "    4. ATTENDANCE AND TIME TRACKING:\n",
    "    - Attendance after holidays\n",
    "    - Absence patterns by department or job level\n",
    "    - Attendance compliance analysis\n",
    "\n",
    "    5. EMPLOYMENT STATUS CHANGES:\n",
    "    - Promotion/demotion rates\n",
    "    - Contract renewals and conversions\n",
    "    - Department transfer patterns\n",
    "\n",
    "    6. DEMOGRAPHIC ANALYSIS:\n",
    "    - Age distribution\n",
    "    - Gender representation by department or job level\n",
    "    - Diversity metrics across locations\n",
    "\n",
    "    7. EDUCATION AND EXPERIENCE:\n",
    "    - Education level distribution\n",
    "    - Qualification analysis by job title\n",
    "    - Prior experience correlation with job placement\n",
    "\n",
    "    8. FAMILY AND PERSONAL:\n",
    "    - Family composition analysis\n",
    "    - Marital status distribution\n",
    "    - Family relation type patterns\n",
    "\n",
    "    9. EMPLOYMENT CONTRACTS:\n",
    "    - Contract type distribution\n",
    "    - Contract duration analysis\n",
    "    - Employment type trends over time\n",
    "\n",
    "    10. ORGANIZATIONAL STRUCTURE:\n",
    "        - Department size comparison\n",
    "        - Location distribution analysis\n",
    "        - Job level distribution by organization\n",
    "\n",
    "    11. FINANCIAL ANALYSIS:\n",
    "        - Salary payment tracking\n",
    "        - Bank transfer analysis\n",
    "        - Bonus and additional income distribution\n",
    "        - Total compensation analysis by organizational unit\n",
    "\n",
    "    12. CUSTOM DATA FIELDS:\n",
    "        - Custom metrics tracking and comparison\n",
    "        - Employee variable analysis\n",
    "        - Trend analysis of custom data fields\n",
    "\n",
    "    13. TIME-BASED ANALYSIS:\n",
    "        - Year-to-date comparisons\n",
    "        - Month-over-month trends\n",
    "        - Quarterly analysis\n",
    "        - Custom date range reporting\n",
    "\n",
    "    14. DATA VISUALIZATION SUPPORT:\n",
    "        - Queries formatted for chart generation\n",
    "        - Time-series data for trend visualization\n",
    "        - Aggregated data for dashboard displays\n",
    "\n",
    "    ## TASK\n",
    "    IMPORTANT: You MUST generate EXACTLY {total_questions} distinct business questions with their corresponding SQL queries. No more, no less. The questions should:\n",
    "\n",
    "    1. Cover all major tables and relationships in the database\n",
    "    2. Range across different complexity levels:\n",
    "    - Basic level (33 percent of questions):\n",
    "        * Simple queries with 1-2 tables\n",
    "        * Simple WHERE conditions\n",
    "        * Basic aggregations (COUNT, SUM, AVG)\n",
    "        * Simple ORDER BY and GROUP BY\n",
    "    - Intermediate level (33 percent of questions):\n",
    "        * 2-4 tables with multiple joins\n",
    "        * Subqueries\n",
    "        * Window functions\n",
    "        * More complex filtering\n",
    "    - Advanced level (34 percent of questions):\n",
    "        * 3-4+ tables with complex joins\n",
    "        * Nested subqueries\n",
    "        * Complex queries with CTEs\n",
    "        * Hierarchical data queries\n",
    "        * Multiple subqueries\n",
    "        * Advanced SQL features\n",
    "    3. Include various SQL concepts such as:\n",
    "        - Simple selection and filtering\n",
    "        - Multiple table joins (INNER, LEFT, RIGHT, FULL OUTER)\n",
    "        - Aggregation functions (COUNT, SUM, AVG, MIN, MAX)\n",
    "        - Grouping and having clauses\n",
    "        - Subqueries (correlated and non-correlated)\n",
    "        - Common Table Expressions (CTEs) and recursive CTEs\n",
    "        - Window functions (ROW_NUMBER, RANK, DENSE_RANK, NTILE, LAG, LEAD)\n",
    "        - Date-based operations and filtering\n",
    "        - Conditional logic (CASE statements, COALESCE, NULLIF)\n",
    "        - Hierarchical data handling (manager-employee relationships)\n",
    "        - Historical data analysis (using employment status histories)\n",
    "        - Set operations (UNION, INTERSECT, EXCEPT)\n",
    "        - String functions and pattern matching (LIKE, REGEXP)\n",
    "        - Pivot and unpivot operations\n",
    "        - Temporary tables and table variables\n",
    "        - Data type conversions and casting\n",
    "        - Advanced grouping (ROLLUP, CUBE, GROUPING SETS)\n",
    "    4. Format the queries appropriately for your database, using the following standards:\n",
    "        - Use STR_TO_DATE for date formatting and comparison\n",
    "        - Always include standard organization, job level, and location filters where applicable\n",
    "        - Use appropriate date functions for date calculations\n",
    "        - Format query output in Bahasa Indonesia where needed\n",
    "        - Include the organization_id, job_level_id, and location_id filtering using the placeholder format: IN ([ORGANIZATION_IDS]), IN ([JOB_LEVEL_IDS]), IN ([LOCATION_IDS])\n",
    "\n",
    "    ## LINGUISTIC VARIATION REQUIREMENTS\n",
    "    To ensure diverse question phrasing, use a wide variety of question structures such as:\n",
    "    1. Direct questions: \"Siapa karyawan dengan...\"\n",
    "    2. Comparative questions: \"Bagaimana perbandingan antara...\"\n",
    "    3. Trend analysis: \"Bagaimana tren perekrutan selama...\"\n",
    "    4. Ranking questions: \"Urutkan departemen berdasarkan...\"\n",
    "    5. Percentage-based: \"Berapa persentase karyawan yang...\"\n",
    "    6. Ratio questions: \"Apa rasio antara karyawan...\"\n",
    "    7. Time-based analysis: \"Kapan terjadi lonjakan...\"\n",
    "    8. Conditional questions: \"Dalam kondisi apa departemen...\"\n",
    "    9. Outlier identification: \"Identifikasi karyawan yang...\"\n",
    "    10. Distribution questions: \"Bagaimana distribusi gaji...\"\n",
    "    11. Correlation questions: \"Adakah korelasi antara...\"\n",
    "    12. Projection questions: \"Proyeksikan jumlah karyawan...\"\n",
    "    13. Threshold questions: \"Temukan departemen dengan tingkat turnover di atas...\"\n",
    "    14. Pattern identification: \"Pola apa yang terlihat dalam...\"\n",
    "    15. Anomaly detection: \"Temukan anomali dalam data...\"\n",
    "\n",
    "    ## OUTPUT FORMAT\n",
    "    Provide the output in valid JSON format as follows. The examples below are just for reference - YOUR QUESTIONS MUST BE COMPLETELY DIFFERENT:\n",
    "\n",
    "    ```json\n",
    "    {{\n",
    "    \"sql_questions\": [\n",
    "        {{\n",
    "            \"question_id\": 1,\n",
    "            \"category\": \"EMPLOYMENT ANALYSIS\",\n",
    "            \"complexity\": \"Intermediate\",\n",
    "            \"business_question\": \"Bagaimana perbandingan jumlah karyawan berdasarkan organisasi? Munculkan nama organisasi dan total karyawan.\",\n",
    "            \"required_tables\": [\"employees\", \"employment_statuses\", \"organizations\"],\n",
    "            \"sql_concepts\": [\"JOIN\", \"COUNT\", \"GROUP BY\", \"WHERE\"],\n",
    "            \"sql_query\": \"SELECT organizations.name AS \\\"organization_name\\\", COUNT(employees.id) AS \\\"total_employees\\\" FROM employees JOIN employment_statuses ON employees.id = employment_statuses.employee_id JOIN organizations ON employment_statuses.organization_id = organizations.id WHERE employees.active = TRUE AND employment_statuses.organization_id IN ('[ORGANIZATION_IDS]') AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]') AND employment_statuses.location_id IN ('[LOCATION_IDS]') GROUP BY organizations.name;\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question_id\": 2,\n",
    "            \"category\": \"TURNOVER ANALYSIS\",\n",
    "            \"complexity\": \"Intermediate\",\n",
    "            \"business_question\": \"Bagaimana data termination berdasarkan nama jabatan?\",\n",
    "            \"required_tables\": [\"termination_entries\", \"employees\", \"employment_statuses\", \"job_titles\"],\n",
    "            \"sql_concepts\": [\"JOIN\", \"COUNT\", \"GROUP BY\", \"WHERE\"],\n",
    "            \"sql_query\": \"SELECT job_titles.name AS \\\"job_title_name\\\", COUNT(termination_entries.id) AS \\\"termination_count\\\" FROM termination_entries JOIN employees ON termination_entries.employee_id = employees.id JOIN employment_statuses ON employees.id = employment_statuses.employee_id JOIN job_titles ON employment_statuses.job_title_id = job_titles.id WHERE termination_entries.approval_status = 'APPROVED' AND employment_statuses.organization_id IN ('[ORGANIZATION_IDS]') AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]') AND employment_statuses.location_id IN ('[LOCATION_IDS]') GROUP BY job_titles.name;\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question_id\": 3,\n",
    "            \"category\": \"TURNOVER ANALYSIS\",\n",
    "            \"complexity\": \"Basic\",\n",
    "            \"business_question\": \"Berikan nama employee beserta termination reasonnya untuk data termination yang belum di approve\",\n",
    "            \"required_tables\": [\"termination_entries\", \"employees\", \"termination_reasons\", \"employment_statuses\"],\n",
    "            \"sql_concepts\": [\"JOIN\", \"LEFT JOIN\", \"WHERE\"],\n",
    "            \"sql_query\": \"SELECT employees.name AS \\\"employee_name\\\", termination_reasons.name AS \\\"Termination Reason\\\" FROM termination_entries JOIN employees ON termination_entries.employee_id = employees.id LEFT JOIN termination_reasons ON termination_entries.termination_reason_id = termination_reasons.id JOIN employment_statuses ON employees.id = employment_statuses.employee_id WHERE termination_entries.approval_status <> 'APPROVED' AND employment_statuses.organization_id IN ('[ORGANIZATION_IDS]') AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]') AND employment_statuses.location_id IN ('[LOCATION_IDS]');\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question_id\": 4,\n",
    "            \"category\": \"FINANCIAL ANALYSIS\",\n",
    "            \"complexity\": \"Intermediate\",\n",
    "            \"business_question\": \"Berikan total transfer untuk bank 'ALLO BANK' di bulan Oktober 2023. Tampilkan total transfer nya saja.\",\n",
    "            \"required_tables\": [\"salary_payment_summaries\", \"company_bank_accounts\", \"bank_branches\", \"banks\"],\n",
    "            \"sql_concepts\": [\"JOIN\", \"SUM\", \"WHERE\", \"BETWEEN\", \"STR_TO_DATE\"],\n",
    "            \"sql_query\": \"SELECT SUM(salary_payment_summaries.transferred_amount) AS 'total_transfer' FROM salary_payment_summaries JOIN company_bank_accounts ON salary_payment_summaries.company_bank_account_id = company_bank_accounts.id JOIN bank_branches ON company_bank_accounts.bank_branch_id = bank_branches.id JOIN banks ON bank_branches.bank_id = banks.id WHERE banks.name = 'ALLO BANK' AND salary_payment_summaries.payment_date BETWEEN STR_TO_DATE('2023-10-01', '%Y-%m-%d') AND STR_TO_DATE('2023-10-31', '%Y-%m-%d');\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question_id\": 5,\n",
    "            \"category\": \"FINANCIAL ANALYSIS\",\n",
    "            \"complexity\": \"Intermediate\",\n",
    "            \"business_question\": \"Berapa total transfer dari bank 'ALLO BANK' dan 'BCA' selama tahun 2023?\",\n",
    "            \"required_tables\": [\"salary_payment_summaries\", \"company_bank_accounts\", \"bank_branches\", \"banks\"],\n",
    "            \"sql_concepts\": [\"JOIN\", \"SUM\", \"WHERE\", \"IN\", \"BETWEEN\", \"STR_TO_DATE\", \"GROUP BY\"],\n",
    "            \"sql_query\": \"SELECT banks.name AS 'bank_name', SUM(salary_payment_summaries.transferred_amount) AS 'total_transferred_amount' FROM salary_payment_summaries JOIN company_bank_accounts ON salary_payment_summaries.company_bank_account_id = company_bank_accounts.id JOIN bank_branches ON company_bank_accounts.bank_branch_id = bank_branches.id JOIN banks ON bank_branches.bank_id = banks.id WHERE banks.name IN ('ALLO BANK', 'BCA') AND salary_payment_summaries.payment_date BETWEEN STR_TO_DATE('2023-01-01', '%Y-%m-%d') AND STR_TO_DATE('2023-12-31', '%Y-%m-%d') GROUP BY banks.name;\"\n",
    "        }},\n",
    "        {{\n",
    "            \"question_id\": 6,\n",
    "            \"category\": \"FINANCIAL ANALYSIS\",\n",
    "            \"complexity\": \"Intermediate\",\n",
    "            \"business_question\": \"Berapa total uang yang ditransfer untuk karyawan yang aktif di bulan Oktober 2023?\",\n",
    "            \"required_tables\": [\"salary_payment_summaries\", \"salary_payments\", \"employees\", \"employment_statuses\"],\n",
    "            \"sql_concepts\": [\"JOIN\", \"SUM\", \"WHERE\", \"BETWEEN\", \"STR_TO_DATE\"],\n",
    "            \"sql_query\": \"SELECT SUM(salary_payment_summaries.transferred_amount) AS 'total_transferred_amount' FROM salary_payment_summaries JOIN salary_payments ON salary_payment_summaries.id = salary_payments.salary_payment_summary_id JOIN employees ON salary_payments.employee_id = employees.id JOIN employment_statuses ON employees.id = employment_statuses.employee_id WHERE employees.active = TRUE AND employment_statuses.organization_id IN ([ORGANIZATION_IDS]) AND employment_statuses.job_level_id IN ([JOB_LEVEL_IDS]) AND employment_statuses.location_id IN ([LOCATION_IDS]) AND salary_payment_summaries.payment_date BETWEEN STR_TO_DATE('2023-10-01', '%Y-%m-%d') AND STR_TO_DATE('2023-10-31', '%Y-%m-%d');\"\n",
    "        }},\n",
    "        // The rest questions here...\n",
    "    ]\n",
    "}}\n",
    "\n",
    "IMPORTANT REQUIREMENTS:\n",
    "1. DO NOT copy the example questions - create completely new ones\n",
    "2. EVERY question must include ALL required fields (question_id, category, complexity, business_question, required_tables, sql_concepts, sql_query)\n",
    "3. Ensure exactly {total_questions} questions are generated\n",
    "4. Maintain the exact JSON structure shown above\n",
    "5. Verify that each question has the correct complexity level to maintain the 33/33/34 percent distribution\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-chat https://api.deepseek.com/v1 sk-672c188728734ae5bd09d1b6b30139a2\n"
     ]
    }
   ],
   "source": [
    "class QnAPair(BaseModel):\n",
    "    paraphrased_input: str = Field(description=\"paraphrased_input\")\n",
    "    test_to_sql_query: str = Field(description=\"test_to_sql_query\")\n",
    "\n",
    "class QnAPairs(BaseModel):\n",
    "    qna_pairs: List[QnAPair] = Field(description=\"list of qna pairs\")\n",
    "\n",
    "## OpenAI / DeepSeek\n",
    "DEEPSEEK_MODEL_NAME = os.getenv(\"DEEPSEEK_MODEL\")\n",
    "DEEPSEEK_ENDPOINT = os.getenv(\"DEEPSEEK_ENDPOINT\")\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "print(DEEPSEEK_MODEL_NAME, DEEPSEEK_ENDPOINT, DEEPSEEK_API_KEY)\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model_name=DEEPSEEK_MODEL_NAME,\n",
    "    temperature=0.7,  # Higher temperature (0.7-0.9) for more creative variations\n",
    "    openai_api_base=DEEPSEEK_ENDPOINT,\n",
    "    openai_api_key=DEEPSEEK_API_KEY,\n",
    "    top_p=0.95,  # Keep high top_p for diverse outputs while filtering unlikely tokens\n",
    "    seed=42  # Optional: set seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create prompt template\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(SYSTEM_MESSAGE)\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(USER_MESSAGE)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message_prompt,\n",
    "    human_message_prompt\n",
    "])\n",
    "parser = JsonOutputParser(pydantic_object=QnAPairs)\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:32:55,396 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sql_questions': [{'question_id': 1,\n",
       "   'category': 'DEMOGRAPHIC ANALYSIS',\n",
       "   'complexity': 'Advanced',\n",
       "   'business_question': 'Bagaimana distribusi usia karyawan berdasarkan jenis kelamin dan tingkat pekerjaan? Tampilkan kelompok usia (20-29, 30-39, 40-49, 50+), jenis kelamin, dan nama tingkat pekerjaan.',\n",
       "   'required_tables': ['employees',\n",
       "    'employee_details',\n",
       "    'job_levels',\n",
       "    'employment_statuses'],\n",
       "   'sql_concepts': ['JOIN',\n",
       "    'CASE',\n",
       "    'GROUP BY',\n",
       "    'DATE functions',\n",
       "    'Multiple table joins'],\n",
       "   'sql_query': \"WITH employee_age AS (\\n    SELECT \\n        e.id,\\n        ed.gender,\\n        jl.name AS job_level_name,\\n        TIMESTAMPDIFF(YEAR, ed.date_of_birth, CURDATE()) AS age\\n    FROM employees e\\n    JOIN employee_details ed ON e.id = ed.employee_id\\n    JOIN employment_statuses es ON e.id = es.employee_id\\n    JOIN job_levels jl ON es.job_level_id = jl.id\\n    WHERE e.active = TRUE\\n    AND es.organization_id IN ([ORGANIZATION_IDS])\\n    AND es.job_level_id IN ([JOB_LEVEL_IDS])\\n    AND es.location_id IN ([LOCATION_IDS])\\n)\\nSELECT \\n    CASE \\n        WHEN age BETWEEN 20 AND 29 THEN '20-29'\\n        WHEN age BETWEEN 30 AND 39 THEN '30-39'\\n        WHEN age BETWEEN 40 AND 49 THEN '40-49'\\n        WHEN age >= 50 THEN '50+'\\n    END AS age_group,\\n    gender,\\n    job_level_name,\\n    COUNT(id) AS employee_count\\nFROM employee_age\\nGROUP BY age_group, gender, job_level_name\\nORDER BY age_group, gender, job_level_name;\"}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%d %B %Y\")\n",
    "total_questions = 1\n",
    "database_type = \"employee\"\n",
    "\n",
    "if database_type == \"employee\":\n",
    "    schema = employee_schema\n",
    "    relations = employee_relations\n",
    "    master_data = employee_master_data\n",
    "    data_trustee_tables = data_trustee_employee\n",
    "    master_data = employee_master_data\n",
    "else:\n",
    "    schema = time_management_schema\n",
    "    relations = time_management_relations\n",
    "    master_data = time_management_master_data\n",
    "    data_trustee_tables = data_trustee_time_management\n",
    "    master_data = time_management_master_data\n",
    "\n",
    "response = chain.invoke({\n",
    "    \"schema\": schema,\n",
    "    \"relations\": relations,\n",
    "    \"master_data\": master_data,\n",
    "    \"data_trustee_tables\": data_trustee_tables,\n",
    "    \"anonymized_entities_description\": anonymized_entities_description,\n",
    "    \"current_date\": current_date,\n",
    "    \"total_questions\": total_questions\n",
    "})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate SQL Question and SQL Query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnName:\n",
    "    NO = \"No\"\n",
    "    PROMPT = \"Prompt\"\n",
    "    COMPLEXITY = \"Complexity\"\n",
    "    CATEGORY = \"Category\"\n",
    "    REQUIRED_TABLES = \"Required Tables\"\n",
    "    SQL_CONCEPTS = \"SQL Concepts\"\n",
    "    EXPECTED_SQL_QUERY = \"Expected SQL Query\"\n",
    "    GENERATED_SQL_QUERY = \"Generated SQL Query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:33:31,389 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 1: 'business_question'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:36:08,550 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2: Generated 18 unique questions (Total: 18/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:38:47,504 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3: Generated 17 unique questions (Total: 35/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:41:24,205 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: Generated 15 unique questions (Total: 50/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:44:03,033 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5: Generated 18 unique questions (Total: 68/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:46:40,858 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 6: Generated 14 unique questions (Total: 82/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:49:18,418 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 7: Generated 14 unique questions (Total: 96/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:51:56,632 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8: Generated 13 unique questions (Total: 109/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:54:36,514 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 9: 'business_question'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 00:57:15,795 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 10: 'business_question'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:00:03,950 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 11: Generated 15 unique questions (Total: 124/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:02:41,059 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-11 01:04:48,419 - INFO - Retrying request to /chat/completions in 0.402129 seconds\n",
      "2025-04-11 01:04:49,499 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 12: Generated 15 unique questions (Total: 139/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:07:25,854 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 13: Generated 21 unique questions (Total: 160/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:10:05,067 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 14: Generated 19 unique questions (Total: 179/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:12:41,632 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 15: Generated 14 unique questions (Total: 193/200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:15:17,479 - INFO - HTTP Request: POST https://api.deepseek.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 16: Generated 7 unique questions (Total: 200/200)\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def generate_questions_in_batches(total_questions: int, batch_size: int = 25, database_type: str = \"employee\") -> list:\n",
    "    \"\"\"Generate SQL questions in batches with deduplication to avoid token limit issues.\n",
    "\n",
    "    Args:\n",
    "        total_questions (int): Total number of questions to generate\n",
    "        batch_size (int): Number of questions to generate in each batch\n",
    "\n",
    "    Returns:\n",
    "        list: Combined list of all generated questions with no duplicates\n",
    "    \"\"\"\n",
    "    all_questions = []\n",
    "    generated_question_texts = set()  # Track question text to avoid duplicates\n",
    "\n",
    "    # Calculate initial number of batches\n",
    "    remaining_questions = total_questions\n",
    "    batch_num = 0\n",
    "\n",
    "    # Continue until we have enough unique questions\n",
    "    while len(all_questions) < total_questions:\n",
    "        batch_num += 1\n",
    "        # Calculate questions for this batch\n",
    "        questions_in_batch = min(batch_size, remaining_questions)\n",
    "\n",
    "        # Update the prompt to request a specific range of question IDs\n",
    "        start_id = len(all_questions) + 1\n",
    "\n",
    "        # Create a list of already generated questions to avoid duplicates\n",
    "        existing_questions_summary = \"\"\n",
    "        if all_questions:\n",
    "            # Create a summary of existing questions (limited to avoid token issues)\n",
    "            sample_questions = all_questions[-min(10, len(all_questions)):]  # Last 10 questions\n",
    "            existing_questions_summary = \"PREVIOUSLY GENERATED QUESTIONS (DO NOT DUPLICATE THESE):\\n\" + \"\\n\".join([\n",
    "                f\"{q['question_id']}. {q['business_question']}\" for q in sample_questions\n",
    "            ])\n",
    "\n",
    "        # Modify the USER_MESSAGE to include batch information and anti-duplication instructions\n",
    "        batch_user_message = USER_MESSAGE.replace(\n",
    "            \"{total_questions}\", str(questions_in_batch)\n",
    "        )\n",
    "\n",
    "        if existing_questions_summary:\n",
    "            batch_user_message += f\"\\n\\n{existing_questions_summary}\\n\\nIMPORTANT: Generate COMPLETELY NEW questions that are different from the above. Do not duplicate concepts, metrics, or specific analysis approaches.\"\n",
    "\n",
    "        # Create batch-specific prompt\n",
    "        batch_system_message = SystemMessagePromptTemplate.from_template(SYSTEM_MESSAGE)\n",
    "        batch_human_message = HumanMessagePromptTemplate.from_template(batch_user_message)\n",
    "        batch_prompt = ChatPromptTemplate.from_messages([\n",
    "            batch_system_message,\n",
    "            batch_human_message\n",
    "        ])\n",
    "\n",
    "        # Create chain and invoke\n",
    "        batch_chain = batch_prompt | llm | parser\n",
    "        current_date = datetime.now().strftime(\"%d %B %Y\")\n",
    "\n",
    "        if database_type == \"employee\":\n",
    "            schema = employee_schema\n",
    "            relations = employee_relations\n",
    "            master_data = employee_master_data\n",
    "            data_trustee_tables = data_trustee_employee\n",
    "            master_data = employee_master_data\n",
    "        else:\n",
    "            schema = time_management_schema\n",
    "            relations = time_management_relations\n",
    "            master_data = time_management_master_data\n",
    "            data_trustee_tables = data_trustee_time_management\n",
    "            master_data = time_management_master_data\n",
    "\n",
    "        try:\n",
    "            response = batch_chain.invoke({\n",
    "                \"schema\": schema,\n",
    "                \"relations\": relations,\n",
    "                \"master_data\": master_data,\n",
    "                \"data_trustee_tables\": data_trustee_tables,\n",
    "                \"anonymized_entities_description\": anonymized_entities_description,\n",
    "                \"current_date\": current_date,\n",
    "                \"total_questions\": questions_in_batch\n",
    "            })\n",
    "\n",
    "            # Filter out duplicate questions\n",
    "            unique_questions = []\n",
    "            for question in response['sql_questions']:\n",
    "                # Normalize the question text for comparison (remove spaces, lowercase)\n",
    "                normalized_text = question['business_question'].lower().replace(\" \", \"\")\n",
    "\n",
    "                # Check if this is a new unique question\n",
    "                if normalized_text not in generated_question_texts:\n",
    "                    # Adjust question ID\n",
    "                    question['question_id'] = start_id + len(unique_questions)\n",
    "                    unique_questions.append(question)\n",
    "                    generated_question_texts.add(normalized_text)\n",
    "\n",
    "            # Add to our collection\n",
    "            all_questions.extend(unique_questions)\n",
    "\n",
    "            print(f\"Batch {batch_num}: Generated {len(unique_questions)} unique questions (Total: {len(all_questions)}/{total_questions})\")\n",
    "\n",
    "            # Update remaining questions\n",
    "            remaining_questions = total_questions - len(all_questions)\n",
    "\n",
    "            # If we didn't get any unique questions in this batch, we might be stuck\n",
    "            if len(unique_questions) == 0 and batch_num > 10:\n",
    "                print(\"Warning: Not generating any new unique questions. Breaking loop.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num}: {e}\")\n",
    "            # Continue with next batch even if this one fails\n",
    "\n",
    "    return all_questions[:total_questions]  # Ensure we don't return more than requested\n",
    "\n",
    "# Generate all questions using the enhanced batching function\n",
    "total_questions = 200\n",
    "# database_type = \"employee\"\n",
    "database_type = \"time\"\n",
    "all_generated_questions = generate_questions_in_batches(total_questions, database_type=database_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question_id': 50, 'category': 'EMPLOYMENT ANALYSIS', 'complexity': 'Intermediate', 'business_question': 'Berapa lama rata-rata karyawan tetap di perusahaan berdasarkan jenis kontrak kerja awal mereka?'}\n",
      "{'question_id': 82, 'category': 'EMPLOYEE HIRING PATTERNS', 'complexity': 'Advanced', 'business_question': 'Bagaimana pola rekrutmen karyawan baru berdasarkan jenis kontrak kerja dan departemen selama 3 tahun terakhir,'}\n",
      "{'question_id': 109, 'category': 'EMPLOYMENT ANALYSIS', 'complexity': 'Intermediate', 'business_question': 'Berapa lama rata-rata karyawan bertahan di'}\n",
      "{'question_id': 193, 'category': 'PRESENCE ANALYSIS', 'complexity': 'Advanced', 'business_question': 'Bagaimana tingkat kehadiran karyawan (persentase hadir vs tidak hadir) per minggu selama 3 bulan terakhir?', 'required_tables': ['presence_entries', 'attendance_statuses'], 'sql_concepts': ['CTE', 'DATE functions', 'C']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Category</th>\n",
       "      <th>Complexity</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Required Tables</th>\n",
       "      <th>SQL Concepts</th>\n",
       "      <th>Generated SQL Query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EMPLOYEE HIRING PATTERNS</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Berapa jumlah karyawan baru yang direkrut seti...</td>\n",
       "      <td>[employees]</td>\n",
       "      <td>[COUNT, GROUP BY, YEAR, MONTH, WHERE]</td>\n",
       "      <td>SELECT MONTH(join_date) AS month, COUNT(*) AS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>ATTENDANCE AND TIME TRACKING</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Berapa rata-rata keterlambatan karyawan (dalam...</td>\n",
       "      <td>[attendance_detail_recapitulations, shifts]</td>\n",
       "      <td>[JOIN, AVG, GROUP BY, WHERE, DATE_FORMAT]</td>\n",
       "      <td>SELECT DATE_FORMAT(date, '%Y-%m') AS month, AV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>EMPLOYMENT STATUS CHANGES</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Berapa jumlah promosi (perubahan job level) ya...</td>\n",
       "      <td>[employment_status_histories, job_levels]</td>\n",
       "      <td>[JOIN, COUNT, GROUP BY, QUARTER, WHERE]</td>\n",
       "      <td>SELECT QUARTER(effective_date) AS quarter, COU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>DEMOGRAPHIC ANALYSIS</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Bagaimana distribusi usia karyawan saat ini be...</td>\n",
       "      <td>[employee_details, employment_statuses, organi...</td>\n",
       "      <td>[JOIN, FLOOR, GROUP BY, WHERE]</td>\n",
       "      <td>SELECT organizations.name AS department, FLOOR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>EDUCATION AND EXPERIENCE</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Apa distribusi tingkat pendidikan karyawan ber...</td>\n",
       "      <td>[educations, education_levels, employment_stat...</td>\n",
       "      <td>[JOIN, COUNT, GROUP BY]</td>\n",
       "      <td>SELECT job_titles.name AS job_title, education...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>196</td>\n",
       "      <td>EMPLOYMENT CONTRACTS</td>\n",
       "      <td>Intermediate</td>\n",
       "      <td>Berapa jumlah karyawan kontrak yang akan berak...</td>\n",
       "      <td>[employment_statuses, employment_types, employ...</td>\n",
       "      <td>[JOIN, COUNT, DATE_ADD, GROUP BY, WHERE, BETWEEN]</td>\n",
       "      <td>SELECT employment_types.name AS contract_type,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>197</td>\n",
       "      <td>DEMOGRAPHIC ANALYSIS</td>\n",
       "      <td>Basic</td>\n",
       "      <td>Bagaimana distribusi karyawan berdasarkan golo...</td>\n",
       "      <td>[employee_details, employees]</td>\n",
       "      <td>[JOIN, COUNT, GROUP BY, WHERE]</td>\n",
       "      <td>SELECT employee_details.blood_type, employee_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>198</td>\n",
       "      <td>MANAGEMENT HIERARCHY</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>Berapa jumlah karyawan yang memiliki lebih dar...</td>\n",
       "      <td>[employees, employment_statuses, locations]</td>\n",
       "      <td>[SUBQUERY, JOIN, COUNT, GROUP BY, HAVING, WHERE]</td>\n",
       "      <td>SELECT m.name AS manager_name, COUNT(e.id) AS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>199</td>\n",
       "      <td>EDUCATION AND EXPERIENCE</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>Bagaimana korelasi antara tingkat pendidikan d...</td>\n",
       "      <td>[educations, education_levels, job_experiences...</td>\n",
       "      <td>[JOIN, AVG, DATEDIFF, GROUP BY, WHERE, SUBQUERY]</td>\n",
       "      <td>SELECT education_levels.name AS education_leve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>200</td>\n",
       "      <td>TIME-BASED ANALYSIS</td>\n",
       "      <td>Advanced</td>\n",
       "      <td>Bagaimana tren penggunaan cuti karyawan selama...</td>\n",
       "      <td>[attendances, attendance_statuses, employees]</td>\n",
       "      <td>[JOIN, COUNT, DATE_FORMAT, GROUP BY, WHERE, BE...</td>\n",
       "      <td>SELECT DATE_FORMAT(attendances.date, '%Y-%m') ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      No                      Category    Complexity  \\\n",
       "0      1      EMPLOYEE HIRING PATTERNS         Basic   \n",
       "1      2  ATTENDANCE AND TIME TRACKING         Basic   \n",
       "2      3     EMPLOYMENT STATUS CHANGES  Intermediate   \n",
       "3      4          DEMOGRAPHIC ANALYSIS  Intermediate   \n",
       "4      5      EDUCATION AND EXPERIENCE  Intermediate   \n",
       "..   ...                           ...           ...   \n",
       "191  196          EMPLOYMENT CONTRACTS  Intermediate   \n",
       "192  197          DEMOGRAPHIC ANALYSIS         Basic   \n",
       "193  198          MANAGEMENT HIERARCHY      Advanced   \n",
       "194  199      EDUCATION AND EXPERIENCE      Advanced   \n",
       "195  200           TIME-BASED ANALYSIS      Advanced   \n",
       "\n",
       "                                                Prompt  \\\n",
       "0    Berapa jumlah karyawan baru yang direkrut seti...   \n",
       "1    Berapa rata-rata keterlambatan karyawan (dalam...   \n",
       "2    Berapa jumlah promosi (perubahan job level) ya...   \n",
       "3    Bagaimana distribusi usia karyawan saat ini be...   \n",
       "4    Apa distribusi tingkat pendidikan karyawan ber...   \n",
       "..                                                 ...   \n",
       "191  Berapa jumlah karyawan kontrak yang akan berak...   \n",
       "192  Bagaimana distribusi karyawan berdasarkan golo...   \n",
       "193  Berapa jumlah karyawan yang memiliki lebih dar...   \n",
       "194  Bagaimana korelasi antara tingkat pendidikan d...   \n",
       "195  Bagaimana tren penggunaan cuti karyawan selama...   \n",
       "\n",
       "                                       Required Tables  \\\n",
       "0                                          [employees]   \n",
       "1          [attendance_detail_recapitulations, shifts]   \n",
       "2            [employment_status_histories, job_levels]   \n",
       "3    [employee_details, employment_statuses, organi...   \n",
       "4    [educations, education_levels, employment_stat...   \n",
       "..                                                 ...   \n",
       "191  [employment_statuses, employment_types, employ...   \n",
       "192                      [employee_details, employees]   \n",
       "193        [employees, employment_statuses, locations]   \n",
       "194  [educations, education_levels, job_experiences...   \n",
       "195      [attendances, attendance_statuses, employees]   \n",
       "\n",
       "                                          SQL Concepts  \\\n",
       "0                [COUNT, GROUP BY, YEAR, MONTH, WHERE]   \n",
       "1            [JOIN, AVG, GROUP BY, WHERE, DATE_FORMAT]   \n",
       "2              [JOIN, COUNT, GROUP BY, QUARTER, WHERE]   \n",
       "3                       [JOIN, FLOOR, GROUP BY, WHERE]   \n",
       "4                              [JOIN, COUNT, GROUP BY]   \n",
       "..                                                 ...   \n",
       "191  [JOIN, COUNT, DATE_ADD, GROUP BY, WHERE, BETWEEN]   \n",
       "192                     [JOIN, COUNT, GROUP BY, WHERE]   \n",
       "193   [SUBQUERY, JOIN, COUNT, GROUP BY, HAVING, WHERE]   \n",
       "194   [JOIN, AVG, DATEDIFF, GROUP BY, WHERE, SUBQUERY]   \n",
       "195  [JOIN, COUNT, DATE_FORMAT, GROUP BY, WHERE, BE...   \n",
       "\n",
       "                                   Generated SQL Query  \n",
       "0    SELECT MONTH(join_date) AS month, COUNT(*) AS ...  \n",
       "1    SELECT DATE_FORMAT(date, '%Y-%m') AS month, AV...  \n",
       "2    SELECT QUARTER(effective_date) AS quarter, COU...  \n",
       "3    SELECT organizations.name AS department, FLOOR...  \n",
       "4    SELECT job_titles.name AS job_title, education...  \n",
       "..                                                 ...  \n",
       "191  SELECT employment_types.name AS contract_type,...  \n",
       "192  SELECT employee_details.blood_type, employee_d...  \n",
       "193  SELECT m.name AS manager_name, COUNT(e.id) AS ...  \n",
       "194  SELECT education_levels.name AS education_leve...  \n",
       "195  SELECT DATE_FORMAT(attendances.date, '%Y-%m') ...  \n",
       "\n",
       "[196 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated 196 unique questions\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame from all questions\n",
    "df_syntetic_data = pd.DataFrame(columns=[\n",
    "    ColumnName.NO,\n",
    "    ColumnName.CATEGORY,\n",
    "    ColumnName.COMPLEXITY,\n",
    "    ColumnName.PROMPT,\n",
    "    ColumnName.REQUIRED_TABLES,\n",
    "    ColumnName.SQL_CONCEPTS,\n",
    "    ColumnName.GENERATED_SQL_QUERY\n",
    "])\n",
    "\n",
    "\n",
    "for qna_pair in all_generated_questions:\n",
    "    try:\n",
    "        new_row = {\n",
    "            ColumnName.NO: qna_pair['question_id'],\n",
    "            ColumnName.CATEGORY: qna_pair['category'],\n",
    "            ColumnName.COMPLEXITY: qna_pair['complexity'],\n",
    "            ColumnName.PROMPT: qna_pair['business_question'],\n",
    "            ColumnName.REQUIRED_TABLES: qna_pair['required_tables'],\n",
    "            ColumnName.SQL_CONCEPTS: qna_pair['sql_concepts'],\n",
    "            ColumnName.GENERATED_SQL_QUERY: qna_pair['sql_query']\n",
    "        }\n",
    "        df_syntetic_data = pd.concat([df_syntetic_data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    except:\n",
    "        print(qna_pair)\n",
    "\n",
    "# Save to CSV\n",
    "from pathlib import Path\n",
    "syntetic_data_dir = Path('sql_generator_result')\n",
    "syntetic_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "df_syntetic_data.to_csv(syntetic_data_dir / 'time_200_batch_2.csv', index=False)\n",
    "display(df_syntetic_data)\n",
    "print(f\"Successfully generated {len(df_syntetic_data)} unique questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>paraphrased_input</th>\n",
       "      <th>ner_label</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_case</td>\n",
       "      <td>pemimpin perusahaan kami bernama Ahmad Fauzi</td>\n",
       "      <td>pemimpin perusahaan kami bernama &lt;PER&gt;Ahmad Fa...</td>\n",
       "      <td>nama bos saya adalah On Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_case</td>\n",
       "      <td>direktur utama di kantor saya adalah Siti Rahayu</td>\n",
       "      <td>direktur utama di kantor saya adalah &lt;PER&gt;Siti...</td>\n",
       "      <td>nama bos saya adalah On Lee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      source                                 paraphrased_input  \\\n",
       "0  test_case      pemimpin perusahaan kami bernama Ahmad Fauzi   \n",
       "1  test_case  direktur utama di kantor saya adalah Siti Rahayu   \n",
       "\n",
       "                                           ner_label  \\\n",
       "0  pemimpin perusahaan kami bernama <PER>Ahmad Fa...   \n",
       "1  direktur utama di kantor saya adalah <PER>Siti...   \n",
       "\n",
       "                       context  \n",
       "0  nama bos saya adalah On Lee  \n",
       "1  nama bos saya adalah On Lee  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_syntetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]2025-01-17 20:24:02,563 - INFO - Successfully wrote row 1/2\n",
      "2025-01-17 20:24:04,427 - INFO - Successfully wrote row 2/2\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.90s/it]\n",
      "2025-01-17 20:24:04,441 - INFO - Successfully wrote 2 rows\n"
     ]
    }
   ],
   "source": [
    "from module.google_sheets_writer import GoogleSheetsWriter\n",
    "import logging\n",
    "\n",
    "\n",
    "writer = GoogleSheetsWriter(\n",
    "    google_util=google,  # Your GoogleUtil instance\n",
    "    sheet_id=GOOGLE_SPREADSHEET_ID,\n",
    "    worksheet_name=SYNTETIC_DATA_SHEET_NAME,\n",
    "    batch_size=10,  # Customize batch size\n",
    "    max_retries=5,  # Customize retry attempts\n",
    "    batch_delay=2  # Customize delay between batches\n",
    ")\n",
    "# Write the DataFrame\n",
    "result = writer.write_dataframe(df_syntetic_data)\n",
    "\n",
    "# Log results\n",
    "logging.info(f\"Successfully wrote {result.successful_rows} rows\")\n",
    "if result.failed_rows > 0:\n",
    "    logging.error(f\"Failed to write {result.failed_rows} rows\")\n",
    "    for error in result.errors:\n",
    "        logging.error(f\"Row {error['row_number']}: {error['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
