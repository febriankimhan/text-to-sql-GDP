{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Indonesian Dataset Using Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**\n",
    "1. Alfan Dinda Rahmawan (alfan.d.rahmawan@gdplabs.id)\n",
    "\n",
    "**References:**\n",
    "1. [Fine-Tuning SDK](https://docs.glair.ai/generative-internal/modules/model-training/llm/supervised-fine-tuning)\n",
    "2. [Training Dataset](https://docs.google.com/spreadsheets/d/1E1V7jFa8vrWwJqNaPwcpUN1DxR46jpWxbgV50cxR1sU/edit?gid=1049327042#gid=1049327042)\n",
    "3. [Evaluation Dataset](https://docs.google.com/spreadsheets/d/1Ql_1M-1Qa6Js0K8M8Ai46TpJdfVT0vGlGCo2yGPeUss/edit?gid=1119237474#gid=1119237474)\n",
    "\n",
    "**Overview**\n",
    "This notebook demonstrates the process of fine-tuning a language model for text to sql using Unsloth. The notebook is organized into four main sections:\n",
    "\n",
    "1. [**Model Preparations**](#model-preparations): \n",
    "   - Setting up the environment and required dependencies.\n",
    "   - Configuring the base model.\n",
    "   - Defining hyperparameters and model configurations.\n",
    "\n",
    "2. [**Data Preparation**](#data-preparation):\n",
    "   - Loading training and validation data from Google Sheets.\n",
    "   - Preprocessing the data for fine-tuning.\n",
    "   - Creating dataset loaders.\n",
    "\n",
    "3. [**Run fine tuning**](#run-fine-tuning):\n",
    "   - Training the model using SFTTrainer.\n",
    "   - Monitoring training metrics and GPU usage.\n",
    "   - Saving the fine-tuned model and hyperparameters.\n",
    "\n",
    "4. [**Sanity Check**](#sanity-check):\n",
    "   - Loading the fine-tuned model.\n",
    "   - Running inference on test cases sample.\n",
    "\n",
    "5. [**Save model to S3**](#save-model-to-s3):\n",
    "   - Zipping the model files.\n",
    "   - Uploading the model to AWS S3 for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment\n",
    "\n",
    "**To install the SDK library, you need to create a personal access token on GitHub. Please follow these steps:**\n",
    "1. You need to log in to your [GitHub Account](https://github.com/).\n",
    "2. Go to the [Personal Access Tokens](https://github.com/settings/tokens) page.\n",
    "3. If you haven't created a Personal Access Tokens yet, you can generate one.\n",
    "4. When generating a new token, make sure that you have checked the `repo` option to grant access to private repositories.\n",
    "5. Now, you can copy the new token that you have generated and paste it into the script below.\n",
    "\n",
    "**To install the model, you need to create a Hugging Face token. Please follow these steps:**\n",
    "1. You need to log in to your [Hugging Face Account](https://huggingface.co/).\n",
    "2. Go to the [Personal Access Tokens](https://huggingface.co/settings/tokens) page.\n",
    "3. If you haven't created a Personal Access Tokens yet, you can generate one.\n",
    "4. Now, you can copy the new token that you have generated and paste it into the script below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_SPREADSHEET_ID: str = \"1dDMqrol_DrEMjvLy88IRu2WdHN7T5BU0LrD8ORLuNPI\" # put your spreadsheet id here\n",
    "GOOGLE_SPREADSHEET_URL: str = f\"https://docs.google.com/spreadsheets/d/{GOOGLE_SPREADSHEET_ID}/edit?usp=sharing\" # put your spreadsheet link here\n",
    "TRAIN_SHEET_NAME: str = \"train_data\"\n",
    "VALIDATION_SHEET_NAME: str = \"validation_data\"\n",
    "\n",
    "GOOGLE_SHEETS_CLIENT_EMAIL: str = os.getenv('GOOGLE_SHEETS_CLIENT_EMAIL')\n",
    "GOOGLE_SHEETS_PRIVATE_KEY: str = os.getenv('GOOGLE_SHEETS_PRIVATE_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Authentication\n",
    "from modules.google_sheets_writer import GoogleUtil\n",
    "\n",
    "PRIVATE_KEY = GOOGLE_SHEETS_PRIVATE_KEY\n",
    "google: GoogleUtil = GoogleUtil(PRIVATE_KEY, GOOGLE_SHEETS_CLIENT_EMAIL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# import getpass\n",
    "# HF_TOKEN = getpass.getpass(\"Insert your Hugging Face Token (your typing will be hidden, press Enter when done): \")\n",
    "# login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Set the model repository name (change this to the desired model)\n",
    "# model_repo = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "# # branch = \"refs/pr/1\"\n",
    "# branch = \"main\"\n",
    "# # Define the directory where the model will be saved\n",
    "# save_directory = os.path.join(\"models\", model_repo)\n",
    "\n",
    "# # Create the directory if it does not exist\n",
    "# os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# # Use the Hugging Face CLI to download the model\n",
    "# download_cmd = f\"huggingface-cli download {model_repo} --revision {branch} --local-dir {save_directory}\"\n",
    "# # os.system(download_cmd)\n",
    "# # prompt = \"huggingface-cli download Ellbendls/Qwen-2.5-3b-Text_to_SQL --local-dir models/Ellbendls/Qwen-2.5-3b-Text_to_SQL\"\n",
    "# # print(f\"Model downloaded to {save_directory}\")\n",
    "# download_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a SQL generator expert for MariaDB 10.5.23. Your task is to create SQL queries based on database schema, table relationships, master data, current date, and user instructions.\n",
    "\n",
    "1. CORE SQL REQUIREMENTS:\n",
    "   - Generate directly executable MariaDB 10.5.23 SQL\n",
    "   - DO NOT select identifiers (id, employee_id) except within aggregate functions (MAX, SUM, AVG)\n",
    "   - Always use human-readable names (religions.name instead of religions.id)\n",
    "   - Prefix all column names with table names to avoid ambiguity\n",
    "   - Use snake_case for column aliases in SELECT clause only (no aliases in FROM)\n",
    "   - Handle dates properly:\n",
    "     * Wrap all date literals in STR_TO_DATE()\n",
    "     * Ensure date comparisons use CAST() for DATE type\n",
    "   - Use aggregate functions ONLY in SELECT or HAVING clauses (never in WHERE)\n",
    "   - Ensure JOIN conditions reference correct foreign keys\n",
    "   - Add extra JOINs to master tables if display names are missing\n",
    "   - Handle division by checking for non-zero denominators\n",
    "   - Name output columns based on user's instruction language\n",
    "\n",
    "2. DATA TRUSTEE REQUIREMENTS:\n",
    "   - If any table in the query appears in the Data Trustee Enabled Tables list:\n",
    "     * JOIN with the employment_statuses table using employee_id column\n",
    "     * Add these exact filters:\n",
    "       employment_statuses.organization_id IN ('[ORGANIZATION_IDS]')\n",
    "       AND employment_statuses.job_level_id IN ('[JOB_LEVEL_IDS]')\n",
    "       AND employment_statuses.location_id IN ('[LOCATION_IDS]')\n",
    "     * If table requires prerequisite join (indicated in data_trustee_tables),\n",
    "       perform that JOIN before joining with employment_statuses\n",
    "   - If no table from the Data Trustee Enabled Tables is used, do not include\n",
    "     any data trustee-specific JOINs or filters\n",
    "\n",
    "3. OUTPUT FORMAT:\n",
    "   - Return ONLY a JSON object with your SQL query in this exact format:\n",
    "        {{\n",
    "        \"sql_query\": \"<your_generated_sql_query>\"\n",
    "        }}\n",
    "   - No explanations or commentary\n",
    "\n",
    "Examples:\n",
    "Input: \"Berapa total uang yang ditransfer untuk karyawan yang aktif di bulan Oktober 2023?\"\n",
    "\n",
    "Output: {\n",
    "  \"sql_query\": [\"SELECT SUM(salary_payment_summaries.transferred_amount) AS 'total_transferred_amount' FROM salary_payment_summaries JOIN salary_payments ON salary_payment_summaries.id = salary_payments.salary_payment_summary_id JOIN employees ON salary_payments.employee_id = employees.id JOIN employment_statuses ON employees.id = employment_statuses.employee_id WHERE employees.active = TRUE AND employment_statuses.organization_id IN ([ORGANIZATION_IDS]) AND employment_statuses.job_level_id IN ([JOB_LEVEL_IDS]) AND employment_statuses.location_id IN ([LOCATION_IDS]) AND salary_payment_summaries.payment_date BETWEEN STR_TO_DATE('2023-10-01', '%Y-%m-%d') AND STR_TO_DATE('2023-10-31', '%Y-%m-%d');\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Generate a SQL query for the following instruction:\n",
    "\n",
    "DATABASE INFORMATION:\n",
    "Schema: {schema}\n",
    "Relationships: {relations}\n",
    "Master Data: {master_data}\n",
    "Data Trustee Enabled Tables: {data_trustee_tables}\n",
    "Anonymized Entities: {anonymized_entities_description}\n",
    "Current Date: {current_date}\n",
    "\n",
    "user_instruction: {user_instruction}\n",
    "Return only the SQL query as a JSON object: {{\"sql_query\": \"<your_sql_query>\"}}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "MAX_SEQ_LENGTH = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "DTYPE = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "LOAD_IN_4BIT = False  # Use 4bit quantization to reduce memory usage\n",
    "\n",
    "# PEFT (Parameter Efficient Fine-Tuning) Configuration\n",
    "PEFT_CONFIG = {\n",
    "    \"r\": 8,                                    # LoRA rank: 8, 16, 32, 64, 128\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0.1,                       # 0 is optimized\n",
    "    \"bias\": \"none\",                            # \"none\" is optimized\n",
    "    \"target_modules\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    \"use_gradient_checkpointing\": \"unsloth\",   # Use for very long context\n",
    "    \"random_state\": 3407,\n",
    "    \"use_rslora\": False,                       # Rank stabilized LoRA support\n",
    "    \"loftq_config\": None,                      # LoftQ support\n",
    "}\n",
    "\n",
    "# SFT (Supervised Fine-Tuning) Configuration\n",
    "SFT_TRAINER_ARGS = {\n",
    "    \"dataset_text_field\": \"prompt\",\n",
    "    \"dataset_num_proc\": 2,\n",
    "    \"packing\": False,                          # Can make training 5x faster for short sequences\n",
    "}\n",
    "\n",
    "# Training Configuration\n",
    "TRAINING_ARGS = {\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"warmup_steps\": 5,\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"learning_rate\": 2e-5,                     # 0.00002 written in scientific notation\n",
    "    \"optim\": \"adamw_8bit\",\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"lr_scheduler_type\": \"linear\",             # Options: constant, linear, cosine, cosine_with_restarts\n",
    "    \"seed\": 42,\n",
    "    \"report_to\": \"none\",                       # Can be modified for WandB integration\n",
    "    \"save_strategy\": \"steps\",                  # Save after a certain number of steps\n",
    "    \"eval_strategy\": \"steps\",                  # Evaluate after a certain number of steps\n",
    "    \"load_best_model_at_end\": True,            # Load the best model based on evaluation metrics\n",
    "    \"save_steps\": 20,                          # Save the model every 20 steps\n",
    "    \"eval_steps\": 20,                          # Evaluate every 20 steps\n",
    "    \"logging_steps\": 20,                       # Log every 20 steps\n",
    "    \"save_total_limit\": 5,                     # Limit the number of saved checkpoints\n",
    "    \"metric_for_best_model\":\"eval_loss\",       # Track \"eval_loss\" to choose the best model\n",
    "    \"greater_is_better\":False,                 # Lower eval_loss is better\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/text_to_sql/fine_tune_exp_1/Qwen/Qwen2.5-0.5B-Instruct/exp_id_1:fine_tuning_ft-1:ft_text_to_sql_few_shot_1:Qwen2.5-0.5B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class ColumnName:\n",
    "    NO = \"No\"\n",
    "    SQL_QUESTION = \"Prompt\"\n",
    "    SQL_QUERY = \"Expected SQL Query\"\n",
    "    DATABASE_TYPE = \"Database\"\n",
    "\n",
    "# Model Paths\n",
    "MODEL_DIR = \"Qwen\"\n",
    "MODEL_NAME = \"Qwen2.5-0.5B-Instruct\"\n",
    "STUDENT_MODEL_NAME = os.path.join(MODEL_DIR, MODEL_NAME)\n",
    "\n",
    "# Get current working directory\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# Experiment Information\n",
    "EXPERIMENT_ID = \"1\"\n",
    "FINETUNED_HYPERPARAM_ID = \"ft-1\"\n",
    "EVALUATION_DIR = \"evaluation\"\n",
    "EXPERIMENT_REASON = f\"Fine tuned text to sql used {STUDENT_MODEL_NAME} model with hyperparam {FINETUNED_HYPERPARAM_ID}, {TRAINING_ARGS['num_train_epochs']} Epoch\"\n",
    "EXPERIMENT_ID_REF = \"-\"\n",
    "PROMPT_ID = \"ft_text_to_sql_few_shot_1\"\n",
    "TOPIC = \"TEXT TO SQL\"\n",
    "\n",
    "FINETUNE_DIR = f\"fine_tune_exp_{EXPERIMENT_ID}\"\n",
    "FINETUNE_OUTPUT_PATH = os.path.join(BASE_DIR, FINETUNE_DIR, STUDENT_MODEL_NAME)\n",
    "FINAL_FINETUNE_OUTPUT_NAME = f\"exp_id_{EXPERIMENT_ID}:fine_tuning_{FINETUNED_HYPERPARAM_ID}:{PROMPT_ID}:{MODEL_NAME}\"\n",
    "\n",
    "DATA_TRAIN_DIR = os.path.join(BASE_DIR, \"data_finetuned\", MODEL_NAME, \"train\")\n",
    "DATA_TRAIN_NAME = f\"exp-{EXPERIMENT_ID}-finetuned-{FINETUNED_HYPERPARAM_ID}-text-to-sql-{MODEL_NAME}.csv\"\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(FINETUNE_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_TRAIN_DIR, exist_ok=True)\n",
    "\n",
    "FINAL_MODEL_PATH = f\"{FINETUNE_OUTPUT_PATH}/{FINAL_FINETUNE_OUTPUT_NAME}\"\n",
    "print(FINAL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsloth preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-24 01:08:09,252 - INFO - PyTorch version 2.6.0 available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.3.18: Fast Qwen2 patching. Transformers: 4.50.0.dev0.\n",
      "   \\\\   /|    NVIDIA RTX A5000. Num GPUs = 1. Max memory: 23.679 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = STUDENT_MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = PEFT_CONFIG[\"r\"], # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = PEFT_CONFIG[\"target_modules\"],\n",
    "    lora_alpha = PEFT_CONFIG[\"lora_alpha\"],\n",
    "    lora_dropout = PEFT_CONFIG[\"lora_dropout\"], # Supports any, but = 0 is optimized\n",
    "    bias = PEFT_CONFIG[\"bias\"],    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = PEFT_CONFIG[\"use_gradient_checkpointing\"], # True or \"unsloth\" for very long context\n",
    "    random_state = PEFT_CONFIG[\"random_state\"],\n",
    "    use_rslora = PEFT_CONFIG[\"use_rslora\"],  # We support rank stabilized LoRA\n",
    "    loftq_config = PEFT_CONFIG[\"loftq_config\"], # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Expected SQL Query</th>\n",
       "      <th>Sheet</th>\n",
       "      <th>Database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Berapa persentase karyawan yang mengajukan pen...</td>\n",
       "      <td>SELECT (COUNT(DISTINCT termination_entries.emp...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Berapa jumlah karyawan baru yang direkrut seti...</td>\n",
       "      <td>SELECT CONCAT(YEAR(employees.join_date), ' Q',...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Siapa manajer dengan jumlah bawahan langsung t...</td>\n",
       "      <td>SELECT employees.name AS 'nama_manajer', COUNT...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bagaimana distribusi usia karyawan di departem...</td>\n",
       "      <td>SELECT FLOOR(DATEDIFF(STR_TO_DATE('06 March 20...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Berapa persentase karyawan kontrak yang diperp...</td>\n",
       "      <td>WITH contract_employees AS (SELECT employees.i...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No                                             Prompt  \\\n",
       "0  1  Berapa persentase karyawan yang mengajukan pen...   \n",
       "1  2  Berapa jumlah karyawan baru yang direkrut seti...   \n",
       "2  3  Siapa manajer dengan jumlah bawahan langsung t...   \n",
       "3  4  Bagaimana distribusi usia karyawan di departem...   \n",
       "4  5  Berapa persentase karyawan kontrak yang diperp...   \n",
       "\n",
       "                                  Expected SQL Query  \\\n",
       "0  SELECT (COUNT(DISTINCT termination_entries.emp...   \n",
       "1  SELECT CONCAT(YEAR(employees.join_date), ' Q',...   \n",
       "2  SELECT employees.name AS 'nama_manajer', COUNT...   \n",
       "3  SELECT FLOOR(DATEDIFF(STR_TO_DATE('06 March 20...   \n",
       "4  WITH contract_employees AS (SELECT employees.i...   \n",
       "\n",
       "                       Sheet Database  \n",
       "0  catapa_syntetics_employee     core  \n",
       "1  catapa_syntetics_employee     core  \n",
       "2  catapa_syntetics_employee     core  \n",
       "3  catapa_syntetics_employee     core  \n",
       "4  catapa_syntetics_employee     core  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows: List[list] = google.retrieve_worksheet(GOOGLE_SPREADSHEET_ID, TRAIN_SHEET_NAME)\n",
    "df_train: pd.DataFrame = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Expected SQL Query</th>\n",
       "      <th>Sheet</th>\n",
       "      <th>Database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Berapa jumlah karyawan yang dipromosikan pada ...</td>\n",
       "      <td>SELECT COUNT(DISTINCT employment_status_histor...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Identifikasi manajer dengan rentang kendali te...</td>\n",
       "      <td>SELECT managers.name AS nama_manajer, COUNT(em...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bagaimana perbandingan jumlah karyawan di seti...</td>\n",
       "      <td>SELECT locations.name AS lokasi_kantor, COUNT(...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Berapa jumlah karyawan di setiap departemen?</td>\n",
       "      <td>SELECT \\n organizations.name AS department_nam...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Berapa jumlah karyawan yang melapor langsung k...</td>\n",
       "      <td>SELECT COUNT(DISTINCT employees.id) AS total_k...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No                                             Prompt  \\\n",
       "0  1  Berapa jumlah karyawan yang dipromosikan pada ...   \n",
       "1  2  Identifikasi manajer dengan rentang kendali te...   \n",
       "2  3  Bagaimana perbandingan jumlah karyawan di seti...   \n",
       "3  4       Berapa jumlah karyawan di setiap departemen?   \n",
       "4  5  Berapa jumlah karyawan yang melapor langsung k...   \n",
       "\n",
       "                                  Expected SQL Query  \\\n",
       "0  SELECT COUNT(DISTINCT employment_status_histor...   \n",
       "1  SELECT managers.name AS nama_manajer, COUNT(em...   \n",
       "2  SELECT locations.name AS lokasi_kantor, COUNT(...   \n",
       "3  SELECT \\n organizations.name AS department_nam...   \n",
       "4  SELECT COUNT(DISTINCT employees.id) AS total_k...   \n",
       "\n",
       "                       Sheet Database  \n",
       "0  catapa_syntetics_employee     core  \n",
       "1  catapa_syntetics_employee     core  \n",
       "2  catapa_syntetics_employee     core  \n",
       "3  catapa_syntetics_employee     core  \n",
       "4  catapa_syntetics_employee     core  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows: List[list] = google.retrieve_worksheet(GOOGLE_SPREADSHEET_ID, VALIDATION_SHEET_NAME)\n",
    "df_validation: pd.DataFrame = pd.DataFrame(rows[1:], columns=rows[0])\n",
    "df_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert into dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training data to: train_exp-1-finetuned-ft-1-text-to-sql-Qwen2.5-0.5B-Instruct.csv\n",
      "Saved test data to: validation_exp-1-finetuned-ft-1-text-to-sql-Qwen2.5-0.5B-Instruct.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "2025-03-24 01:17:12,776 - WARNING - Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acad4f2e585e4a038a16efcd4745a67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18078901eacb4eb3986bd525fd6c73e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "2025-03-24 01:17:13,223 - WARNING - Setting num_proc from 64 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8231286a0141c4b1d4c753cec4810e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ce9ac910c347aab94cdd3771274f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"A class for preprocessing datasets for Named Entity Recognition (NER) fine-tuning.\n",
    "\n",
    "This class handles the preprocessing of training and validation datasets for NER tasks,\n",
    "including chat template application, tokenization, and dataset formatting. It supports\n",
    "saving processed datasets and creating dataset loaders for training.\n",
    "\n",
    "Author:\n",
    "    Alfan Dinda Rahmawan (alfan.d.rahmawan@gdplabs.id)\n",
    "\n",
    "References:\n",
    "    -\n",
    "\"\"\"\n",
    "import os\n",
    "from typing import Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from gdplabs_gen_ai_training.sft_trainer import DatasetLoader\n",
    "from gdplabs_gen_ai_training.training_arguments import DataArguments\n",
    "from gdplabs_gen_ai_training.utils import validate_args\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "from modules.database_info.schema import employee_schema, time_management_schema\n",
    "from modules.database_info.master_data import employee_master_data, time_management_master_data\n",
    "from modules.database_info.relation import employee_relations, time_management_relations\n",
    "from modules.database_info.trustee_tables import data_trustee_employee, data_trustee_time_management\n",
    "from modules.database_info.anonymize_entities import anonymized_entities_description\n",
    "\n",
    "class DatasetPreprocessor:\n",
    "    def __init__(\n",
    "        self, df_train: pd.DataFrame, df_validation: pd.DataFrame, tokenizer: PreTrainedTokenizer, system_prompt: str,\n",
    "        base_filename: str, dir_path: str, user_field_name: str, assistant_field_name: str, prompt_key_name: str, test_size: float = 0.1, random_state: int = 42\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the DatasetPreprocessor.\n",
    "\n",
    "        Args:\n",
    "            df_train (pd.DataFrame): Input dataset\n",
    "            df_validation (pd.DataFrame): Input dataset\n",
    "            tokenizer: The tokenizer to use\n",
    "            system_prompt (str): System prompt for the model\n",
    "            base_filename (str): Base name for output files\n",
    "            dir_path (str): Directory path for output files\n",
    "            test_size (float): Proportion of dataset to use for test split\n",
    "            random_state (int): Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.df_train = df_train\n",
    "        self.df_validation = df_validation\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = system_prompt\n",
    "        self.base_filename = base_filename\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.dir_path = dir_path\n",
    "        self.user_field_name = user_field_name\n",
    "        self.assistant_field_name = assistant_field_name\n",
    "        self.prompt_key_name = prompt_key_name\n",
    "\n",
    "    def _process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process a dataframe by applying the chat template and formatting.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to process\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed DataFrame\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        tmp_df = pd.DataFrame()\n",
    "        current_date = datetime.now().strftime(\"%d %B %Y\")\n",
    "        for row in df.iterrows():\n",
    "            row_data = row[1]\n",
    "            sql_question = row_data[ColumnName.SQL_QUESTION]\n",
    "            database_type = row_data[ColumnName.DATABASE_TYPE]\n",
    "\n",
    "            # Invoke the chain with the current batch\n",
    "            if database_type == \"employee\":\n",
    "                schema = employee_schema\n",
    "                relations = employee_relations\n",
    "                master_data = employee_master_data\n",
    "                data_trustee_tables = data_trustee_employee\n",
    "                master_data = employee_master_data\n",
    "            else:\n",
    "                schema = time_management_schema\n",
    "                relations = time_management_relations\n",
    "                master_data = time_management_master_data\n",
    "                data_trustee_tables = data_trustee_time_management\n",
    "                master_data = time_management_master_data\n",
    "\n",
    "            user_message = USER_PROMPT.format(\n",
    "                user_instruction=sql_question,\n",
    "                schema=schema,\n",
    "                relations=relations,\n",
    "                master_data=master_data,\n",
    "                data_trustee_tables=data_trustee_tables,\n",
    "                anonymized_entities_description=anonymized_entities_description,\n",
    "                current_date=current_date\n",
    "            )\n",
    "        df['user_message'] = user_message\n",
    "        df[self.prompt_key_name] = df.apply(\n",
    "            lambda row: self.tokenizer.apply_chat_template(\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": row['user_message']},\n",
    "                    {\"role\": \"assistant\", \"content\": row[ColumnName.SQL_QUERY]}\n",
    "                ],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False,\n",
    "                continue_final_message=False\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        df[self.prompt_key_name] = df[self.prompt_key_name].str.rstrip('\\n')\n",
    "        df['expected_answer'] = \"\"\n",
    "        return df[[self.prompt_key_name, 'expected_answer']]\n",
    "\n",
    "    def process_and_save(self) -> Tuple[str, str]:\n",
    "        \"\"\"Process both splits and save them to files.\"\"\"\n",
    "\n",
    "        # Process and save training data\n",
    "        train_processed = self._process_dataframe(self.df_train)\n",
    "        train_filename = f\"train_{self.base_filename}\"\n",
    "        train_path = os.path.join(self.dir_path, train_filename)\n",
    "        train_processed.to_csv(train_path, index=False)\n",
    "\n",
    "        # # Process and save test data\n",
    "        validation_processed = self._process_dataframe(self.df_validation)\n",
    "        validation_filename = f\"validation_{self.base_filename}\"\n",
    "        validation_path = os.path.join(self.dir_path, validation_filename)\n",
    "        validation_processed.to_csv(validation_path, index=False)\n",
    "\n",
    "        print(f\"Saved training data to: {train_filename}\")\n",
    "        print(f\"Saved test data to: {validation_filename}\")\n",
    "\n",
    "        return train_path, validation_path\n",
    "\n",
    "    def get_dataset_loader(self, file_path: str) -> DatasetLoader:\n",
    "        \"\"\"\n",
    "        Create and return a DatasetLoader for the training data.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the training data file\n",
    "\n",
    "        Returns:\n",
    "            DatasetLoader: Configured dataset loader\n",
    "        \"\"\"\n",
    "        data_args = validate_args(DataArguments, dict({\"dataset_path\": file_path}))\n",
    "        dataset_loader = DatasetLoader(data_args, self.tokenizer)\n",
    "        dataset_loader.load_and_format_dataset()\n",
    "        return dataset_loader\n",
    "\n",
    "\n",
    "class ColumnName:\n",
    "    NO = \"No\"\n",
    "    SQL_QUESTION = \"Prompt\"\n",
    "    SQL_QUERY = \"Expected SQL Query\"\n",
    "    DATABASE_TYPE = \"Database\"\n",
    "\n",
    "\n",
    "# Initialize the preprocessor\n",
    "preprocessor = DatasetPreprocessor(\n",
    "    df_train=df_train,\n",
    "    df_validation=df_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    system_prompt=SYSTEM_PROMPT,\n",
    "    base_filename=DATA_TRAIN_NAME,\n",
    "    dir_path=DATA_TRAIN_DIR,\n",
    "    user_field_name=ColumnName.SQL_QUESTION,\n",
    "    assistant_field_name=ColumnName.SQL_QUERY,\n",
    "    prompt_key_name=\"prompt\"\n",
    ")\n",
    "\n",
    "# Split, process, and save the datasets\n",
    "train_file, validation_file = (\n",
    "    preprocessor\n",
    "    .process_and_save()\n",
    ")\n",
    "\n",
    "# Get the dataset loader for training\n",
    "dataset_loader_train = preprocessor.get_dataset_loader(train_file)\n",
    "dataset_loader_validation = preprocessor.get_dataset_loader(validation_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd6baeadab04d3c9c0385442e82f599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"prompt\"] (num_proc=2):   0%|          | 0/394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcd561a9f224d179906be966eb40609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"prompt\"] (num_proc=2):   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_loader_train.dataset,\n",
    "    eval_dataset= dataset_loader_validation.dataset,\n",
    "    dataset_text_field = SFT_TRAINER_ARGS[\"dataset_text_field\"],\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc = SFT_TRAINER_ARGS[\"dataset_num_proc\"],\n",
    "    packing = SFT_TRAINER_ARGS[\"packing\"], # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = TRAINING_ARGS[\"per_device_train_batch_size\"],\n",
    "        gradient_accumulation_steps = TRAINING_ARGS[\"gradient_accumulation_steps\"],\n",
    "        warmup_steps = TRAINING_ARGS[\"warmup_steps\"],\n",
    "        num_train_epochs = TRAINING_ARGS[\"num_train_epochs\"], # Set this for 1 full training run.\n",
    "        learning_rate = TRAINING_ARGS[\"learning_rate\"],\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        optim = TRAINING_ARGS[\"optim\"],\n",
    "        weight_decay = TRAINING_ARGS[\"weight_decay\"],\n",
    "        lr_scheduler_type = TRAINING_ARGS[\"lr_scheduler_type\"],  # constant, linear, cosine, cosine_with_restarts\n",
    "        seed = TRAINING_ARGS[\"seed\"],\n",
    "        output_dir = FINETUNE_OUTPUT_PATH,\n",
    "        report_to = TRAINING_ARGS[\"report_to\"], # Use this for WandB etc,\n",
    "        save_strategy=TRAINING_ARGS[\"save_strategy\"],\n",
    "        eval_strategy=TRAINING_ARGS[\"eval_strategy\"],\n",
    "        load_best_model_at_end=TRAINING_ARGS[\"load_best_model_at_end\"],\n",
    "        save_steps=TRAINING_ARGS[\"save_steps\"],\n",
    "        eval_steps=TRAINING_ARGS[\"eval_steps\"],\n",
    "        metric_for_best_model=TRAINING_ARGS[\"metric_for_best_model\"],\n",
    "        greater_is_better=TRAINING_ARGS[\"greater_is_better\"],\n",
    "        logging_steps=TRAINING_ARGS[\"logging_steps\"],\n",
    "        save_total_limit=TRAINING_ARGS[\"save_total_limit\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX A5000. Max memory = 23.679 GB.\n",
      "0.967 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 394 | Num Epochs = 1 | Total steps = 98\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 4,399,104/498,431,872 (0.88% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='98' max='98' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [98/98 02:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.594200</td>\n",
       "      <td>1.476482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.389400</td>\n",
       "      <td>1.304458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.241100</td>\n",
       "      <td>1.177721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.134200</td>\n",
       "      <td>1.094253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best model save to: /home/text_to_sql/fine_tune_exp_1/Qwen/Qwen2.5-0.5B-Instruct/exp_id_1:fine_tuning_ft-1:ft_text_to_sql_few_shot_1:Qwen2.5-0.5B-Instruct\n",
      "CPU times: user 2min 9s, sys: 1.78 s, total: 2min 10s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer_stats = trainer.train()\n",
    "trainer.save_model(FINAL_MODEL_PATH)\n",
    "print(f\"the best model save to: {FINAL_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139.7354 seconds used for training.\n",
      "2.33 minutes used for training.\n",
      "Peak reserved memory = 1.406 GB.\n",
      "Peak reserved memory for training = 0.439 GB.\n",
      "Peak reserved memory % of max memory = 5.938 %.\n",
      "Peak reserved memory for training % of max memory = 1.854 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24/24 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2914953426438935 1.0942529439926147\n"
     ]
    }
   ],
   "source": [
    "# Get the final training loss\n",
    "total_train_loss = sum([log['train_loss'] for log in trainer.state.log_history if 'train_loss' in log])\n",
    "\n",
    "# Get the final validation loss\n",
    "total_validation_loss = trainer.evaluate()[\"eval_loss\"]\n",
    "print(total_train_loss, total_validation_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results to the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp_id</th>\n",
       "      <th>exp_id_ref</th>\n",
       "      <th>experiment_reason</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>student_model</th>\n",
       "      <th>inference_framework</th>\n",
       "      <th>ner_tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>...</th>\n",
       "      <th>num_question_validation</th>\n",
       "      <th>num_question_test</th>\n",
       "      <th>topic</th>\n",
       "      <th>fine_tuned_config</th>\n",
       "      <th>max_training_vram(GB)</th>\n",
       "      <th>GPU Type</th>\n",
       "      <th>training_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>training_time (seconds)</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>Fine tuned text to sql used Qwen/Qwen2.5-0.5B-...</td>\n",
       "      <td>ft_text_to_sql_few_shot_1</td>\n",
       "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>vllm</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td></td>\n",
       "      <td>TEXT TO SQL</td>\n",
       "      <td>{'max_seq_length': 2048, 'dtype': None, 'load_...</td>\n",
       "      <td>1.406</td>\n",
       "      <td>NVIDIA RTX A5000</td>\n",
       "      <td>1.291</td>\n",
       "      <td>1.094</td>\n",
       "      <td>139.7354</td>\n",
       "      <td>2025-03-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  exp_id exp_id_ref                                  experiment_reason  \\\n",
       "0      1          -  Fine tuned text to sql used Qwen/Qwen2.5-0.5B-...   \n",
       "\n",
       "                   prompt_id               student_model inference_framework  \\\n",
       "0  ft_text_to_sql_few_shot_1  Qwen/Qwen2.5-0.5B-Instruct                vllm   \n",
       "\n",
       "  ner_tag precision recall f1_score  ... num_question_validation  \\\n",
       "0                                    ...                      24   \n",
       "\n",
       "  num_question_test        topic  \\\n",
       "0                    TEXT TO SQL   \n",
       "\n",
       "                                   fine_tuned_config max_training_vram(GB)  \\\n",
       "0  {'max_seq_length': 2048, 'dtype': None, 'load_...                 1.406   \n",
       "\n",
       "           GPU Type  training_loss  val_loss training_time (seconds)  \\\n",
       "0  NVIDIA RTX A5000          1.291     1.094                139.7354   \n",
       "\n",
       "         date  \n",
       "0  2025-03-24  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Get GPU type\n",
    "if torch.cuda.is_available():\n",
    "    GPU_TYPE = torch.cuda.get_device_name(0)\n",
    "else:\n",
    "    GPU_TYPE = \"None\"\n",
    "\n",
    "# Hyperparameters\n",
    "hyperparameters_inf = {\n",
    "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
    "    \"dtype\": DTYPE,\n",
    "    \"load_in_4bit\": LOAD_IN_4BIT,\n",
    "    \"peft_config\": PEFT_CONFIG,\n",
    "    \"sft_trainer_args\": SFT_TRAINER_ARGS,\n",
    "    \"training_args\": TRAINING_ARGS\n",
    "}\n",
    "\n",
    "\n",
    "df_leaderboard = pd.DataFrame([\n",
    "    {\n",
    "        'exp_id': EXPERIMENT_ID,\n",
    "        'exp_id_ref': EXPERIMENT_ID_REF,\n",
    "        'experiment_reason': EXPERIMENT_REASON,\n",
    "        'prompt_id': PROMPT_ID,\n",
    "        'student_model': STUDENT_MODEL_NAME,\n",
    "        'inference_framework': \"vllm\",\n",
    "        'ner_tag': \"\",\n",
    "        'precision': \"\",\n",
    "        'recall': \"\",\n",
    "        'f1_score': \"\",\n",
    "        'val_precision': \"\",\n",
    "        'val_recall': \"\",\n",
    "        'val_f1': \"\",\n",
    "        'training_set_id': TRAIN_SHEET_NAME,\n",
    "        'validation_set_id': VALIDATION_SHEET_NAME,\n",
    "        'test_set_id': \"\",\n",
    "        'num_question_train': len(df_train),\n",
    "        'num_question_validation': len(df_validation),\n",
    "        'num_question_test': \"\",\n",
    "        'topic': TOPIC,\n",
    "        'fine_tuned_config': hyperparameters_inf,\n",
    "        'max_training_vram(GB)': used_memory,\n",
    "        'GPU Type': GPU_TYPE,\n",
    "        'training_loss': round(total_train_loss, 3),\n",
    "        'val_loss': round(total_validation_loss, 3),\n",
    "        'training_time (seconds)': round(trainer_stats.metrics['train_runtime'], 4),\n",
    "        'date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    }\n",
    "])\n",
    "df_leaderboard.to_csv(f\"{FINETUNE_OUTPUT_PATH}/leaderboard_result.csv\")\n",
    "df_leaderboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 10:44:32,280 - INFO - Successfully wrote row 1/1\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.06s/it]\n",
      "2025-01-26 10:44:32,285 - INFO - Successfully wrote 1 rows\n"
     ]
    }
   ],
   "source": [
    "# Upload to Leaderboard Google Sheets\n",
    "from modules.google_sheets_writer import GoogleSheetsWriter\n",
    "import logging\n",
    "\n",
    "LEADERBOARD_SHEET_NAME = \"leaderboard-fine-tuned\"\n",
    "writer = GoogleSheetsWriter(\n",
    "    google_util=google,  # Your GoogleUtil instance\n",
    "    sheet_id=GOOGLE_SPREADSHEET_ID,\n",
    "    worksheet_name=LEADERBOARD_SHEET_NAME,\n",
    "    batch_size=10,  # Customize batch size\n",
    "    max_retries=5,  # Customize retry attempts\n",
    "    batch_delay=2  # Customize delay between batches\n",
    ")\n",
    "# Write the DataFrame\n",
    "result = writer.write_dataframe(df_leaderboard)\n",
    "\n",
    "# Log results\n",
    "logging.info(f\"Successfully wrote {result.successful_rows} rows\")\n",
    "if result.failed_rows > 0:\n",
    "    logging.error(f\"Failed to write {result.failed_rows} rows\")\n",
    "    for error in result.errors:\n",
    "        logging.error(f\"Row {error['row_number']}: {error['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/tmp/ipykernel_3666/2846169643.py:32: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  hf_pipeline = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "import torch\n",
    "model_dir = \"unsloth\"  # based on your base model path.\n",
    "model_name = \"Qwen2.5-0.5B-Instruct\" # based on your base model path.\n",
    "model_id = os.path.join(model_dir, model_name)\n",
    "sft_path = \"/home/text_to_sql/fine_tune_exp_1/Qwen/Qwen2.5-0.5B-Instruct/exp_id_1:fine_tuning_ft-1:ft_text_to_sql_few_shot_1:Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, sft_path)  # uncomment if you have fine tuned model\n",
    "model = model.merge_and_unload()  # uncomment if you have fine tuned model\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=model,\n",
    "    device=0,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    tokenizer=tokenizer,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    return_full_text=False,\n",
    "    model_kwargs = {\"temperature\": 0, \"do_sample\":True},\n",
    ")\n",
    "\n",
    "hf_pipeline = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Expected SQL Query</th>\n",
       "      <th>Sheet</th>\n",
       "      <th>Database</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Berapa jumlah karyawan yang dipromosikan pada ...</td>\n",
       "      <td>SELECT COUNT(DISTINCT employment_status_histor...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Identifikasi manajer dengan rentang kendali te...</td>\n",
       "      <td>SELECT managers.name AS nama_manajer, COUNT(em...</td>\n",
       "      <td>catapa_syntetics_employee</td>\n",
       "      <td>core</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  No                                             Prompt  \\\n",
       "0  1  Berapa jumlah karyawan yang dipromosikan pada ...   \n",
       "1  2  Identifikasi manajer dengan rentang kendali te...   \n",
       "\n",
       "                                  Expected SQL Query  \\\n",
       "0  SELECT COUNT(DISTINCT employment_status_histor...   \n",
       "1  SELECT managers.name AS nama_manajer, COUNT(em...   \n",
       "\n",
       "                       Sheet Database  \n",
       "0  catapa_syntetics_employee     core  \n",
       "1  catapa_syntetics_employee     core  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_validation.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL queries:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL queries:   4%|â–         | 1/24 [00:02<01:02,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sql_query\": \"SELECT COUNT(*) AS 'jumlah_karyawan_promosikan' FROM employees WHERE job_level_id IN ([Job Level IDs]) AND employment_status_type_id IN ([Employment Status Type IDs]) AND start_date BETWEEN STR_TO_DATE('2023-01-01', '%Y-%m-%d') AND STR_TO_DATE('2023-12-31', '%Y-%m-%d');\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL queries:   8%|â–Š         | 2/24 [00:15<03:14,  8.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sql_query\": \"SELECT employees.name, MAX(employee_details.race_id) AS 'race_id', MAX(employee_details.religion_id) AS'religion_id', MAX(employee_details.blood_type) AS 'blood_type', MAX(employee_details.marital_status) AS'marital_status', MAX(employee_details.gender) AS 'gender', MAX(employee_details.date_of_birth) AS 'date_of_birth', MAX(employee_details.family_card_number) AS 'family_card_number', MAX(employee_details.race_id) AS 'race_id', MAX(employee_details.religion_id) AS'religion_id', MAX(employee_details.blood_type) AS 'blood_type', MAX(employee_details.marital_status) AS'marital_status', MAX(employee_details.gender) AS 'gender', MAX(employee_details.date_of_birth) AS 'date_of_birth', MAX(employee_details.family_card_number) AS 'family_card_number', MAX(employee_details.race_id) AS 'race_id', MAX(employee_details.religion_id) AS'religion_id', MAX(employee_details.blood_type) AS 'blood_type', MAX(employee_details.marital_status) AS'marital_status', MAX(employee_details.gender) AS 'gender', MAX(employee_details.date_of_birth) AS 'date_of_birth', MAX(employee_details.family_card_number) AS 'family_card_number', MAX(employee_details.race_id) AS 'race_id', MAX(employee_details.religion_id) AS'religion_id', MAX(employee_details.blood_type) AS 'blood_type', MAX(employee_details.marital_status) AS'marital_status', MAX(employee_details.gender) AS 'gender', MAX(employee_details.date_of_birth) AS 'date_of_birth', MAX(employee_details.family_card_number) AS 'family_card_number', MAX(employee_details.race_id) AS 'race_id', MAX(employee_details.religion_id) AS'religion_id', MAX(employee_details.blood_type) AS 'blood_type', MAX(employee_details.marital_status) AS'marital_status', MAX(employee_details.gender) AS 'gender', MAX(employee_details.date_of_birth) AS 'date_of_birth', MAX(employee_details.family_card_number) AS 'family_card_number', MAX(employee_details.race_id) AS 'race_id', MAX(employee_details.religion_id) AS'religion_id', MAX(employee_details.blood_type) AS 'blood_type', MAX(employee_details.marital_status) AS'marital_status', MAX(employee_details.gender) AS 'gender', MAX(employee_details.date_of_birth) AS 'date_of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL queries:  12%|â–ˆâ–Ž        | 3/24 [00:17<01:52,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sql_query\": \"SELECT locations.name AS 'Locais', COUNT(employee_details.employee_id) AS 'Jumlah Employee' FROM locations JOIN employees ON locations.id = employees.location_id GROUP BY locations.name;\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating SQL queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:18<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"sql_query\": \"SELECT COUNT(*) AS 'jumlah_karyawan_langup_dengan_manajer_tingkat_atas' FROM employees WHERE manager_id IS NOT NULL;\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from modules.database_info.schema import employee_schema, time_management_schema\n",
    "from modules.database_info.master_data import employee_master_data, time_management_master_data\n",
    "from modules.database_info.relation import employee_relations, time_management_relations\n",
    "from modules.database_info.trustee_tables import data_trustee_employee, data_trustee_time_management\n",
    "from modules.database_info.anonymize_entities import anonymized_entities_description\n",
    "\n",
    "results = []\n",
    "\n",
    "inference_result_dir = \"sql_generator_result\"\n",
    "os.makedirs(inference_result_dir, exist_ok=True)\n",
    "current_date = datetime.now().strftime(\"%d %B %Y\")\n",
    "sql_generator_result_path = os.path.join(inference_result_dir, f\"{model_name}-using-catapa-prompt.csv\")\n",
    "\n",
    "\n",
    "question_id_test = [1, 2, 3, 5]\n",
    "\n",
    "for index, df_row in tqdm(enumerate(df_validation.iterrows()), desc=\"Generating SQL queries\", total=len(df_validation)):\n",
    "    user_instruction = df_row[1]['Prompt']\n",
    "    no = df_row[1]['No']\n",
    "    if int(no) not in question_id_test:\n",
    "        continue\n",
    "    database_type = df_row[1]['Database']\n",
    "\n",
    "    start_time = time()\n",
    "    if database_type == \"core\":\n",
    "        schema = employee_schema\n",
    "        relations = employee_relations\n",
    "        master_data = employee_master_data\n",
    "        data_trustee_tables = data_trustee_employee\n",
    "        master_data = employee_master_data\n",
    "    else:\n",
    "        schema = time_management_schema\n",
    "        relations = time_management_relations\n",
    "        master_data = time_management_master_data\n",
    "        data_trustee_tables = data_trustee_time_management\n",
    "        master_data = time_management_master_data\n",
    "\n",
    "    formatted_user_prompt = USER_PROMPT.format(\n",
    "        schema=schema,\n",
    "        relations=relations,\n",
    "        master_data=master_data,\n",
    "        data_trustee_tables=data_trustee_tables,\n",
    "        anonymized_entities_description=anonymized_entities_description,\n",
    "        current_date=current_date,\n",
    "        user_instruction=user_instruction,\n",
    "        query_result = \"\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": formatted_user_prompt},\n",
    "    ]\n",
    "    # Check if the tokenizer has a chat template\n",
    "    has_chat_template = hasattr(tokenizer, \"chat_template\") and tokenizer.chat_template is not None\n",
    "\n",
    "    if has_chat_template:\n",
    "        # Tokenize input with chat template\n",
    "        inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        inputs = messages\n",
    "\n",
    "    sql_generator_result = hf_pipeline.invoke(inputs)\n",
    "    print(sql_generator_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE MODEL TO S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/gen-ai-exploration/fine_tuning/sft/ner\n",
      "Changed to: /home/gen-ai-exploration/fine_tuning/sft/ner/fine_tune_results_17/Qwen/Qwen2.5-1.5B-Instruct/exp_id_21:fine_tuning_17:ner_entity_value_few_shot_simple\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Change to the desired directory\n",
    "os.chdir(FINAL_MODEL_PATH)\n",
    "\n",
    "# Verify the change\n",
    "print(\"Changed to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: README.md (deflated 66%)\n",
      "  adding: adapter_model.safetensors (deflated 8%)\n",
      "  adding: adapter_config.json (deflated 55%)\n",
      "  adding: tokenizer_config.json (deflated 83%)\n",
      "  adding: special_tokens_map.json (deflated 67%)\n",
      "  adding: added_tokens.json (deflated 65%)\n",
      "  adding: vocab.json (deflated 61%)\n",
      "  adding: merges.txt (deflated 57%)\n",
      "  adding: tokenizer.json (deflated 81%)\n",
      "  adding: training_args.bin (deflated 52%)\n",
      "  adding: hyperparameters.json (deflated 60%)\n",
      "exp_id_21:fine_tuning_17:ner_entity_value_few_shot_simple.zip\n"
     ]
    }
   ],
   "source": [
    "## Save to zip\n",
    "zip_name = f\"{FINAL_FINETUNE_OUTPUT_NAME}.zip\"\n",
    "!zip -r \"../{zip_name}\" \".\"\n",
    "print(zip_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import getpass\n",
    "\n",
    "# Set your AWS credentials programmatically using boto3\n",
    "aws_access_key = getpass.getpass(\"Insert your AWS Access Key (your typing will be hidden, press Enter when done): \")\n",
    "aws_secret_key = getpass.getpass(\"Insert your AWS Secret Key (your typing will be hidden, press Enter when done): \")\n",
    "region = \"ap-southeast-1\"  # Example region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 10:46:29,771 - INFO - Found credentials in environment variables.\n"
     ]
    }
   ],
   "source": [
    "# Set AWS credentials as environment variables (for the entire notebook session)\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_key\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = region\n",
    "\n",
    "# You can now create clients or resources from this session\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Function to handle the \"aws s3 cp --recursive\" functionality\n",
    "def upload_with_aws_s3_cp(local_dir: str, s3_bucket: str, s3_prefix: str):\n",
    "    command = f\"aws s3 cp --recursive {local_dir} s3://{s3_bucket}/{s3_prefix}\"\n",
    "    os.system(command)  # This will execute the AWS CLI command directly from the notebook\n",
    "\n",
    "def download_with_aws_s3_cp(local_dir: str, s3_bucket: str, s3_prefix: str):\n",
    "    command = f\"aws s3 cp --recursive s3://{s3_bucket}/{s3_prefix} {local_dir}\"\n",
    "    os.system(command)  # This will execute the AWS CLI command directly from the notebook\n",
    "\n",
    "def upload_zip_to_s3(local_file_path: str, s3_bucket: str, s3_prefix: str):\n",
    "    \"\"\"Upload a zip file to S3 bucket\n",
    "\n",
    "    Args:\n",
    "        local_file_path (str): Path to local zip file\n",
    "        s3_bucket (str): Name of S3 bucket\n",
    "        s3_key (str): S3 object key (path in bucket)\n",
    "\n",
    "    Returns:\n",
    "        bool: True if upload successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Upload file to S3\n",
    "        s3.upload_file(local_file_path, s3_bucket, s3_prefix)\n",
    "        print(f\"Successfully uploaded {local_file_path} to s3://{s3_bucket}/{s3_prefix}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading file to S3: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload zip model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded ../exp_id_20:fine_tuning_16:ner_entity_value_few_shot_simple.zip to s3://glair-gen-ai-llm-model/sft_slm_ner/fine-tuned/exp_id_20:fine_tuning_16:ner_entity_value_few_shot_simple.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_model = f\"../{zip_name}\"\n",
    "s3_model_bucket = \"glair-gen-ai-llm-model\"\n",
    "s3_model_prefix = f\"sft_slm_ner/fine-tuned/{zip_name}\"\n",
    "\n",
    "#  Upload zip file\n",
    "upload_zip_to_s3(zip_model, s3_model_bucket, s3_model_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
